{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "\n",
    "# https://towardsdatascience.com/a-beginners-guide-to-graph-neural-networks-using-pytorch-geometric-part-1-d98dc93e7742\n",
    "# https://towardsdatascience.com/a-beginners-guide-to-graph-neural-networks-using-pytorch-geometric-part-2-cd82c01330ab\n",
    "# https://github.com/tkipf/gcn\n",
    "\n",
    "#from torch_geometric.datasets import KarateClub\n",
    "#data = KarateClub()\n",
    "\n",
    "class MultiGraphModel(torch.nn.Module):\n",
    "    def __init__(self, Z, K):\n",
    "        super().__init__()\n",
    "        N,D = Z.shape\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.Z = Z\n",
    "\n",
    "        self.K = K\n",
    "\n",
    "    def sample(self, n, Zs = None):\n",
    "\n",
    "        A = np.zeros((n, self.K, self.N, self.N))\n",
    "        pw_dists = np.linalg.norm(self.Z[:, None, :] - self.Z[None, :, :], axis=-1)\n",
    "\n",
    "        for k in range(self.K):\n",
    "            A[:,k,:,:] = np.random.binomial(n=1, p=np.exp(-np.repeat(pw_dists[np.newaxis,:,:], n, axis=0)))\n",
    "        return A\n",
    "\n",
    "J = 3\n",
    "n_nodes, latent_dim_per_node = 300, 1\n",
    "#Z = np.random.normal(size=(n_nodes,latent_dim_per_node))\n",
    "Z = np.random.random(size=(n_nodes,latent_dim_per_node))\n",
    "TZ = np.concatenate([np.cos(2*np.pi*Z), np.sin(2*np.pi*Z)], axis=1)\n",
    "m = MultiGraphModel(TZ, J)\n",
    "\n",
    "dim_Z_per_node = latent_dim_per_node\n",
    "dim_T_per_node = 2 * dim_Z_per_node\n",
    "dim_Z = dim_Z_per_node * n_nodes\n",
    "dim_T = dim_T_per_node * n_nodes\n",
    "dim_js = [n_nodes for j in range(J)]  # dimensions of marginals\n",
    "\n",
    "graph_comp_mode = 'single'\n",
    "\n",
    "N = 1\n",
    "A = m.sample(n=N)\n",
    "edge_index = np.zeros((N,))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution, LogPartition_gauss_diagonal, LogPartition_discrete, LogPartition_vonMises\n",
    "from rpm import ExpFam, ConditionalExpFam, SemiparametricConditionalExpFam, RPM\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "setup = 'vonMises'\n",
    "\n",
    "if setup == 'gaussian':  # conditional Gaussian case\n",
    "    # define Gaussian prior in natural parametrization  \n",
    "    def activation_out(x,d=dim_Z_per_node): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "        return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "    log_partition = LogPartition_gauss_diagonal(d=dim_Z)\n",
    "elif setup == 'discrete': # conditional categorical case\n",
    "    def activation_out(x,d=None): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "        return torch.nn.LogSoftmax(dim=-1)(x)\n",
    "    log_partition = LogPartition_discrete(D=dim_T)\n",
    "elif setup == 'vonMises': # conditional categorical case\n",
    "    def activation_out(x,d=None): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "        return torch.nn.Identity()(x)\n",
    "    log_partition = LogPartition_vonMises(d=dim_Z)\n",
    "\n",
    "latent_prior = ExpFam(natparam=torch.normal(mean=0.0, std=torch.ones(dim_T).reshape(1,-1)),\n",
    "                                    log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "\n",
    "# GCN model with 2 layers \n",
    "class Net_multigraph(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net_multigraph, self).__init__()\n",
    "        self.conv1 = GCNConv(n_in, n_hidden)\n",
    "        self.conv2 = GCNConv(n_hidden, n_out)\n",
    "        self.activation_out = activation_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.ndim in [2,3]\n",
    "        if x.ndim == 2 or (x.ndim==3 and len(x) ==1):\n",
    "\n",
    "            connectivity = x[0] if len(x) == 1 else x\n",
    "            num_nodes = connectivity.shape[0]\n",
    "            assert connectivity.ndim==2 and connectivity.shape[1] == num_nodes\n",
    "            node_features = torch.eye(num_nodes)\n",
    "            assert node_features.shape[0] == num_nodes\n",
    "            edge_index, edge_weights = torch_geometric.utils.to_edge_index(connectivity.to_sparse())\n",
    "\n",
    "            x = torch.nn.functional.relu(self.conv1(node_features, edge_index))\n",
    "            #x = torch.nn.functional.dropout(x, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.activation_out(x)\n",
    "            return  x.transpose(-2,-1).reshape(1,-1) if x.ndim==3 else x.transpose(-2,-1).flatten() \n",
    "        else:\n",
    "            return torch.stack([self.forward(xn) for xn in x], axis=0)\n",
    "\n",
    "class Net_singlegraph(torch.nn.Module):\n",
    "    def __init__(self, edge_index, node_features, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net_singlegraph, self).__init__()\n",
    "\n",
    "        self.edge_index = edge_index\n",
    "        node_ids = edge_index.flatten().unique()\n",
    "        assert torch.all(node_ids==torch.arange(len(node_ids)))\n",
    "        self.num_nodes = len(node_ids)\n",
    "        self.node_features = torch.eye(self.num_nodes) if node_features is None else node_features\n",
    "        assert self.node_features.shape[0] == self.num_nodes\n",
    "\n",
    "        self.conv1 = GCNConv(self.node_features.shape[-1], n_hidden)\n",
    "        self.conv2 = GCNConv(n_hidden, n_out)\n",
    "        self.activation_out = activation_out\n",
    "\n",
    "    def forward(self, x=None):\n",
    "\n",
    "        x = torch.nn.functional.relu(self.conv1(self.node_features, self.edge_index))\n",
    "        #x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, self.edge_index)\n",
    "        x = self.activation_out(x)\n",
    "        return  x.transpose(-2,-1).reshape(1,-1) \n",
    "\n",
    "\"\"\"\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation_out(x)\n",
    "\"\"\"\n",
    "\n",
    "if graph_comp_mode == 'multi':\n",
    "    xjs = [ torch.tensor(A[:,j],dtype=dtype) for j in range(J)]\n",
    "    natparam_models = [Net_multigraph(dim_js[j], dim_T_per_node, n_hidden=8, activation_out=activation_out) for j in range(J)]\n",
    "elif graph_comp_mode == 'single':\n",
    "    xjs = [ torch_geometric.utils.to_edge_index(torch.tensor(A[0,j],dtype=dtype).to_sparse())[0] for j in range(J)]\n",
    "    natparam_models = [Net_singlegraph(xjs[j], None, dim_T_per_node, n_hidden=8, activation_out=activation_out) for j in range(J)]\n",
    "\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n",
    "rec_factors = [ConditionalExpFam(model=m, log_partition=log_partition) for m in natparam_models]\n",
    "\n",
    "\n",
    "q =  SemiparametricConditionalExpFam(natparams=torch.normal(mean=0.0, std=torch.ones(N, dim_T)), \n",
    "                                     log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "# constsruct implicit RPM\n",
    "rpm = RPM(rec_factors, latent_prior, pxjs, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(p.shape)  for p in natparam_models[0].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152bcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer_p = torch.optim.Adam(rpm.joint_model.parameters(), lr=1e-3)\n",
    "optimizer_q = torch.optim.Adam(rpm.q.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100000\n",
    "batch_size = 1\n",
    "\n",
    "class RPMDatasetMultigraph(torch.utils.data.Dataset):\n",
    "    def __init__(self,xjs,num_features=None):\n",
    "        self.J = len(xjs)\n",
    "        assert all([len(xjs[0]) == len(xjs[j]) for j in range(self.J)])\n",
    "        self.xjs = xjs\n",
    "    def __len__(self):\n",
    "        return len(self.xjs[0])\n",
    "    def __getitem__(self,idx):\n",
    "        print(idx)\n",
    "        return [self.xjs[j][idx] for j in range(self.J)], idx\n",
    "\n",
    "class RPMDatasetSinglegraph(torch.utils.data.Dataset):\n",
    "    def __init__(self,xjs,num_features=None):\n",
    "        self.J = len(xjs)\n",
    "        assert all([len(xjs[0]) == len(xjs[j]) for j in range(self.J)])\n",
    "        self.xjs = xjs\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    def __getitem__(self,idx):\n",
    "        return [torch.zeros((0,1)) for j in range(self.J)], idx\n",
    "\n",
    "def RPMDataset(xjs):\n",
    "    if graph_comp_mode == 'multi':\n",
    "        return RPMDatasetMultigraph(xjs)\n",
    "    elif graph_comp_mode == 'single':\n",
    "        return RPMDatasetSinglegraph(xjs)\n",
    "    else: \n",
    "        RaiseException()\n",
    "\n",
    "ds = RPMDataset(xjs)\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for (batch, idx_data) in dl:\n",
    "        optimizer_p.zero_grad()\n",
    "        optimizer_q.zero_grad()\n",
    "\n",
    "        loss = rpm.training_step(batch, idx_data, batch_idx=t)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_p.step()\n",
    "        optimizer_q.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        print('step #', t, '/', len(ls), ', loss = ', ls[t])\n",
    "        t+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e7e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "if ls.min() < 0:\n",
    "    plt.semilogy( np.arange(t), ls[:t] - ls[:t].min() + 1e-7 )\n",
    "    plt.ylabel('shifted loss (axis shifted to ensure positive values for semilogy)')\n",
    "else:\n",
    "    plt.semilogy( np.arange(t), ls[:t])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b528def",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d81037",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "eta = rpm.q.nat_param(nat_param_offset=rpm.latent_prior.nat_param)\n",
    "angle = torch.arctan2(eta[:,0::2], eta[:,1::2]).detach().numpy()\n",
    "plt.plot(angle[0])\n",
    "\n",
    "r = torch.sqrt((eta.reshape(-1,d,2)**2).sum(axis=-1)).reshape(-1,d,1)\n",
    "mu =  eta.reshape(-1,d,2) * (torch.special.i1(r)/torch.i0(r)/(r+1.*(r==0.)))\n",
    "mu = mu.reshape(-1,d*2)\n",
    "mu = rpm.q.mean_param(nat_param_offset=rpm.latent_prior.nat_param)\n",
    "\n",
    "angle = torch.arctan2(mu[0,0::2], mu[0,1::2]).detach().numpy()\n",
    "plt.plot(angle, ':', color='red')\n",
    "\n",
    "mu = rpm.latent_prior.nat_param\n",
    "angle = torch.arctan2(mu[0,0::2], mu[0,1::2]).detach().numpy()\n",
    "plt.plot(angle, '--', color='green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x = rpm.q.mean_param(nat_param_offset=rpm.latent_prior.nat_param).detach().numpy()\n",
    "\n",
    "plt.plot(Z, mu_x[0, 300:], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74656803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "clrs = matplotlib.cm.get_cmap('hsv')(np.linspace(0,1,n_nodes))[:,:3]\n",
    "\n",
    "mu_x = rpm.q.mean_param(nat_param_offset=rpm.latent_prior.nat_param).detach().numpy().reshape(-1,2)\n",
    "mu_x.shape\n",
    "\n",
    "idx = np.argsort(Z.flatten())\n",
    "for i in range(n_nodes):\n",
    "    plt.plot(Z.flatten()[idx[i]], np.arctan2(*mu_x[idx[i]]), '.', color=clrs[i])\n",
    "    #plt.plot(i, mu_x.flatten()[idx[i]], '.', color=clrs[i])\n",
    "plt.show()\n",
    "\n",
    "mu_sorted = mu_x.flatten()[idx]\n",
    "Z_sorted = Z.flatten()[idx]\n",
    "\n",
    "pwdists_Z = np.sqrt((Z_sorted.reshape(-1,1) - Z_sorted.reshape(1,-1))**2)\n",
    "pwdists_mu = np.sqrt((mu_sorted.reshape(-1,1) - mu_sorted.reshape(1,-1))**2)\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pwdists_mu)\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pwdists_Z)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85375af5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "clrs = matplotlib.cm.get_cmap('hsv')(np.linspace(0,1,n_nodes))[:,:3]\n",
    "\n",
    "p = rpm.q.nat_param(nat_param_offset=rpm.latent_prior.nat_param).detach().numpy()\n",
    "d = rpm.q.log_partition.d\n",
    "mu_x = -0.5 * p[:,:d] / p[:,d:]\n",
    "mu_x.shape\n",
    "\n",
    "for i in range(n_nodes//2):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(mu_x[:,2*i+0], mu_x[:,2*i+1], '.', color=clrs[i])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(Z[i,0], Z[i,1], 'x', color=clrs[i])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "for i in range(n_nodes//2, n_nodes):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(mu_x[:,2*i+0], mu_x[:,2*i+1], '.', color=clrs[i])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(Z[i,0], Z[i,1], 'x', color=clrs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c9d61",
   "metadata": {},
   "source": [
    "# Toy toy-test-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce752a",
   "metadata": {},
   "source": [
    "# Linear-Gaussian true generative model with univariate latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "\n",
    "dim_T = 2\n",
    "J = 3\n",
    "dim_js = [20 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1\n",
    "\n",
    "N = 1000\n",
    "Z_true = np.random.normal(size=(N,dim_Z))\n",
    "A = [-2., 0., 1.]\n",
    "\n",
    "xjs = [A[j] * Z_true + np.random.normal(size=(N,dim_js[j])) for j in range(J)]\n",
    "xjs = [torch.tensor(xj, dtype=torch.float32) for xj in xjs]\n",
    "\n",
    "[xj.shape for xj in xjs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32bac4",
   "metadata": {},
   "source": [
    "# 2D Gaussian copula with Gaussian or Exponential marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "from discreteRPM import discreteRPM, Prior_discrete, RecognitionFactor_discrete\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "J = 2                           # three marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "dim_T = 2                       # dimension of sufficient statistics\n",
    "\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'exponential' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "\n",
    "N = 10000\n",
    "xjs = px.sample_n(n=N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.semilogy( np.arange(t), ls[:t] - ls[:t].min() + 1e-7 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "p = rpm.q.nat_param(nat_param_offset=rpm.latent_prior.nat_param).detach().numpy()\n",
    "mu_x = -0.5 * p[:,0] / p[:,1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(xjs[0][:,0], xjs[1][:,0], mu_x, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c402c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = rpm.q.nat_param(nat_param_offset=rpm.latent_prior.nat_param).detach().numpy()\n",
    "plt.plot(Z_true, -0.5 * p[:,0] / p[:,1], '.')\n",
    "plt.show()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in rpm.joint_model[0].parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
