{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals, LogPartition_gauss\n",
    "from implicitRPM import ImplicitPrior_ExpFam, ImplicitRecognitionFactor_ExpFam, ImplicitRPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "J = 2                           # three marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "dim_T = 2                       # dimension of sufficient statistics\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'exponential' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    #A = np.random.normal(size=(J,J))\n",
    "    #P = A.dot(A.T)\n",
    "    #P = P / np.sqrt(np.outer(np.diag(P), np.diag(P)))    \n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "\n",
    "\n",
    "# define Gaussian prior in natural parametrization  \n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "log_partition_gauss = LogPartition_gauss(d=dim_Z,D=dim_T)\n",
    "latent_prior = ImplicitPrior_ExpFam(natparam=torch.normal(mean=0.0, std=torch.ones(dim_T).reshape(1,-1)),\n",
    "                                    log_partition=log_partition_gauss, activation_out=activation_out)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization  \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "natparam_models = [Net(dim_js[j], dim_T, n_hidden=10, activation_out=activation_out) for j in range(J)]\n",
    "rec_models = [ImplicitRecognitionFactor_ExpFam(model=m, log_partition=log_partition_gauss) for m in natparam_models]\n",
    "\n",
    "# constsruct implicit RPM\n",
    "irpm = ImplicitRPM(rec_models, latent_prior, pxjs)\n",
    "\n",
    "# ahem ... rejection sampling network initializations...\n",
    "while torch.any(torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000)))) == torch.nan):\n",
    "    irpm = ImplicitRPM(rec_models, latent_prior, pxjs)\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc87544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 3:\n",
    "    XX,YY,ZZ = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten(), ZZ.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1), ZZ.reshape(-1,1)]).reshape(100,100,100)\n",
    "elif J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n",
    "\n",
    "if J == 2:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(logpx.detach().numpy(), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    #plt.plot(xjs05[1], xjs05[0], 'ro')\n",
    "    #xjs= pxind.sample_n(50)\n",
    "    #plt.plot(xjs[1], xjs[0], 'kx')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=5)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    #plt.plot(xjs05[1], xjs05[0], 'ro')\n",
    "    #xjs= pxind.sample_n(50)\n",
    "    #plt.plot(xjs[1], xjs[0], 'kx')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c04f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior mean parameters before training\n",
    "log_partition_gauss.grad(irpm.latent_prior.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal mean parameters before training (they'll differ from prior mean above !)\n",
    "[torch.mean(log_partition_gauss.grad(irpm(pxind.sample_n(n=10000))[j]),axis=0) for j in range(irpm.J)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(irpm.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 1000\n",
    "N = 10000\n",
    "batch_size = 1000\n",
    "\n",
    "class RPMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,xjs):\n",
    "        self.J = len(xjs)\n",
    "        assert all([len(xjs[0]) == len(xjs[j]) for j in range(self.J)])\n",
    "        self.xjs = xjs\n",
    "    def __len__(self):\n",
    "        return len(self.xjs[0])\n",
    "    def __getitem__(self,idx):\n",
    "        return [self.xjs[j][idx] for j in range(self.J)]\n",
    "\n",
    "ds = RPMDataset(px.sample_n(n=N))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = irpm.training_step_sm(batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=10000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging - checking if loss coincides with numerical estimate\n",
    "\n",
    "xjs = pxind.sample_n(100)\n",
    "\n",
    "f1 = torch.func.jacrev(irpm.eval_sum, argnums=0)\n",
    "f2 = torch.func.jacrev(f1)\n",
    "loss_auto = 0.\n",
    "loss_auto = loss_auto + 0.5*(f1(xjs)[0]**2 + f1(xjs)[1]**2).squeeze()\n",
    "loss_auto = loss_auto + torch.diag(f2(xjs)[0][0].squeeze()) + torch.diag(f2(xjs)[1][1].squeeze())\n",
    "\n",
    "plt.plot(irpm.loss_sm(xjs).detach())\n",
    "plt.plot(loss_auto.detach(), '--')\n",
    "plt.show()\n",
    "irpm.loss_sm(xjs) - loss_auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior mean after learning\n",
    "log_partition_gauss.grad(irpm.latent_prior.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal mean parameters before training (they should match now, that's what we trained for !)\n",
    "[torch.mean(log_partition_gauss.grad(irpm(pxind.sample_n(n=10000))[j]),axis=0) for j in range(irpm.J)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(xxs[j], torch.exp(pxjs[j].log_prob(xxs[j])).detach().numpy(), label='true p(xj)')\n",
    "    knotj = [k for k in range(J)]\n",
    "    knotj.pop(j)\n",
    "    fac = np.prod([xxs[k].diff()[0] for k in knotj])\n",
    "    plt.plot(xxs[j], fac * torch.sum(torch.exp(logpx),dim=tuple(knotj)).detach().numpy(), label='est. p(xj)')\n",
    "    plt.legend()\n",
    "    plt.title(r'marginals $p(x_j)$')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    xx = torch.linspace((m-3*torch.sqrt(sig2)).detach().numpy()[0], \n",
    "                        (m+3*torch.sqrt(sig2)).detach().numpy()[0], 100)\n",
    "    prior = torch.distributions.normal.Normal(loc=m, scale=torch.sqrt(sig2))\n",
    "    plt.plot(xx.detach(), torch.exp(prior.log_prob(xx)).detach().numpy(), label='p(Z)')\n",
    "    xj = pxind.sample_n(n=1000)[j]\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.rec_models[j](xj)+irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    N = len(m)\n",
    "    def fj(x):\n",
    "        return sum([torch.exp(torch.distributions.normal.Normal(loc=m[n], scale=torch.sqrt(sig2[n])).log_prob(x)) for n in range(N)])/N\n",
    "    plt.plot(xx.detach(), fj(xx).detach().numpy(), label='Fj(Z)')\n",
    "    plt.legend()\n",
    "    plt.title(r'prior $p(Z)$ vs factor evidence $F_j(Z)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45185b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if J == 2:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n",
    "    plt.imshow(logpx.detach().numpy(), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(ds)\n",
    "epochs = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = irpm.training_step_sm(batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=10000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(xxs[j], torch.exp(pxjs[j].log_prob(xxs[j])).detach().numpy(), label='true p(xj)')\n",
    "    knotj = [k for k in range(J)]\n",
    "    knotj.pop(j)\n",
    "    fac = np.prod([xxs[k].diff()[0] for k in knotj])\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n",
    "    plt.plot(xxs[j], fac * torch.sum(torch.exp(logpx),dim=tuple(knotj)).detach().numpy(), label='est. p(xj)')\n",
    "    plt.legend()\n",
    "    plt.title(r'marginals $p(x_j)$')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    xx = torch.linspace((m-3*torch.sqrt(sig2)).detach().numpy()[0], \n",
    "                        (m+3*torch.sqrt(sig2)).detach().numpy()[0], 100)\n",
    "    prior = torch.distributions.normal.Normal(loc=m, scale=torch.sqrt(sig2))\n",
    "    plt.plot(xx.detach(), torch.exp(prior.log_prob(xx)).detach().numpy(), label='p(Z)')\n",
    "    xj = pxind.sample_n(n=1000)[j]\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.rec_models[j](xj)+irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    N = len(m)\n",
    "    def fj(x):\n",
    "        return sum([torch.exp(torch.distributions.normal.Normal(loc=m[n], scale=torch.sqrt(sig2[n])).log_prob(x)) for n in range(N)])/N\n",
    "    plt.plot(xx.detach(), fj(xx).detach().numpy(), label='Fj(Z)')\n",
    "    plt.legend()\n",
    "    plt.title(r'prior $p(Z)$ vs factor evidence $F_j(Z)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if J == 2:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n",
    "    plt.imshow(logpx.detach().numpy(), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(ds)\n",
    "epochs = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = irpm.training_step_sm(batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=10000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    if marginals == 'gaussian':\n",
    "        xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "    elif marginals == 'exponential':\n",
    "        xxs = [torch.linspace(0.001, 10/rates[j],1000) for j in range(J)]\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(1000,1000)\n",
    "    logpx = logpx - torch.log(torch.exp(logpx).sum())    \n",
    "    plt.plot(xxs[j], torch.exp(pxjs[j].log_prob(xxs[j])).detach().numpy(), label='true p(xj)')\n",
    "    knotj = [k for k in range(J)]\n",
    "    knotj.pop(j)\n",
    "    fac = np.prod([xxs[k].diff()[0] for k in knotj])\n",
    "    plt.plot(xxs[j], 1/fac * torch.sum(torch.exp(logpx),dim=tuple(knotj)).detach().numpy(), label='est. p(xj)')\n",
    "    plt.legend()\n",
    "    plt.title(r'marginals $p(x_j)$')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    xx = torch.linspace((m-3*torch.sqrt(sig2)).detach().numpy()[0], \n",
    "                        (m+3*torch.sqrt(sig2)).detach().numpy()[0], 100)\n",
    "    prior = torch.distributions.normal.Normal(loc=m, scale=torch.sqrt(sig2))\n",
    "    plt.plot(xx.detach(), torch.exp(prior.log_prob(xx)).detach().numpy(), label='p(Z)')\n",
    "    xj = pxind.sample_n(n=1000)[j]\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.rec_models[j](xj)+irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    N = len(m)\n",
    "    def fj(x):\n",
    "        return sum([torch.exp(torch.distributions.normal.Normal(loc=m[n], scale=torch.sqrt(sig2[n])).log_prob(x)) for n in range(N)])/N\n",
    "    plt.plot(xx.detach(), fj(xx).detach().numpy(), label='Fj(Z)')\n",
    "    plt.legend()\n",
    "    plt.title(r'prior $p(Z)$ vs factor evidence $F_j(Z)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if J == 2:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(1000,1000)\n",
    "    logpx = logpx - torch.log(torch.exp(logpx).sum())\n",
    "    plt.imshow(logpx.detach().numpy(), origin='lower', extent=(0., 10/rates[1], 0., 10/rates[0]), aspect='auto')\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 10/rates[1], 0., 10/rates[0]))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=10)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under iRPM with copula loss')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71b012",
   "metadata": {},
   "source": [
    "# second part: combine copula loss with maximum likelihood of implicit RPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "irpm0 = copy.deepcopy(irpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4179f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constsruct implicit RPM\n",
    "irpm = ImplicitRPM(rec_models, latent_prior, pxjs)\n",
    "\n",
    "# ahem ... rejection sampling network initializations...\n",
    "while torch.any(torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000)))) == torch.nan):\n",
    "    irpm = ImplicitRPM(rec_models, latent_prior, pxjs)\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(irpm.parameters(), lr=1e-3)\n",
    "T = 1000\n",
    "ls = np.zeros(T)\n",
    "for t in range(T):\n",
    "    optimizer.zero_grad()\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    loss = irpm.training_step(batch=xjs, batch_idx=0, lmbda=10.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ls[t] = loss.detach().numpy()\n",
    "plt.semilogy(ls)\n",
    "plt.show()\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 3:\n",
    "    XX,YY,ZZ = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten(), ZZ.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1), ZZ.reshape(-1,1)]).reshape(100,100,100)\n",
    "elif J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(xxs[j], torch.exp(pxjs[j].log_prob(xxs[j])).detach().numpy(), label='true p(xj)')\n",
    "    knotj = [k for k in range(J)]\n",
    "    knotj.pop(j)\n",
    "    fac = np.prod([xxs[k].diff()[0] for k in knotj])\n",
    "    plt.plot(xxs[j], fac * torch.sum(torch.exp(logpx),dim=tuple(knotj)).detach().numpy(), label='est. p(xj)')\n",
    "    plt.legend()\n",
    "    plt.title(r'marginals $p(x_j)$')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    xx = torch.linspace((m-3*torch.sqrt(sig2)).detach().numpy()[0], \n",
    "                        (m+3*torch.sqrt(sig2)).detach().numpy()[0], 100)\n",
    "    prior = torch.distributions.normal.Normal(loc=m, scale=torch.sqrt(sig2))\n",
    "    plt.plot(xx.detach(), torch.exp(prior.log_prob(xx)).detach().numpy(), label='p(Z)')\n",
    "    xj = xjs[j] #pxind.sample_n(n=1000)[j]\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.rec_models[j](xj)+irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    N = len(m)\n",
    "    def fj(x):\n",
    "        return sum([torch.exp(torch.distributions.normal.Normal(loc=m[n], scale=torch.sqrt(sig2[n])).log_prob(x)) for n in range(N)])/N\n",
    "    plt.plot(xx.detach(), fj(xx).detach().numpy(), label='Fj(Z)')\n",
    "    plt.legend()\n",
    "    plt.title(r'prior $p(Z)$ vs factor evidence $F_j(Z)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [irpm0, irpm]\n",
    "plt.figure(figsize=(12,7))\n",
    "if J == 2:\n",
    "    for i in range(2):\n",
    "        logpx = models[i].eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100).detach().numpy()\n",
    "        plt.subplot(1,2,i+1)\n",
    "        plt.imshow(logpx, origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "        plt.contour(YY, XX , logpx, levels=5)\n",
    "        plt.ylabel(r'$x_1$')\n",
    "        plt.xlabel(r'$x_2$')\n",
    "        xjs = px.sample_n(n=50)\n",
    "        plt.plot(xjs[1], xjs[0], 'r.')\n",
    "        plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "        if i == 0:\n",
    "            plt.title('log p(x) under iRPM with copula loss')\n",
    "        elif i == 1:\n",
    "            plt.title('log p(x) under iRPM with full loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed958a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(irpm.parameters(), lr=1e-3)\n",
    "T = 140\n",
    "ls = np.zeros(T)\n",
    "for t in range(T):\n",
    "    optimizer.zero_grad()\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    loss = irpm.training_step(batch=xjs, batch_idx=0, lmbda=10.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ls[t] = loss.detach().numpy()\n",
    "plt.semilogy(ls)\n",
    "plt.show()\n",
    "torch.mean(torch.exp(irpm.eval(pxind.sample_n(n=1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652901b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 3:\n",
    "    XX,YY,ZZ = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten(), ZZ.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1), ZZ.reshape(-1,1)]).reshape(100,100,100)\n",
    "elif J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    logpx = irpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(xxs[j], torch.exp(pxjs[j].log_prob(xxs[j])).detach().numpy(), label='true p(xj)')\n",
    "    knotj = [k for k in range(J)]\n",
    "    knotj.pop(j)\n",
    "    fac = np.prod([xxs[k].diff()[0] for k in knotj])\n",
    "    plt.plot(xxs[j], fac * torch.sum(torch.exp(logpx),dim=tuple(knotj)).detach().numpy(), label='est. p(xj)')\n",
    "    plt.legend()\n",
    "    plt.title(r'marginals $p(x_j)$')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    xx = torch.linspace((m-3*torch.sqrt(sig2)).detach().numpy()[0], \n",
    "                        (m+3*torch.sqrt(sig2)).detach().numpy()[0], 100)\n",
    "    prior = torch.distributions.normal.Normal(loc=m, scale=torch.sqrt(sig2))\n",
    "    plt.plot(xx.detach(), torch.exp(prior.log_prob(xx)).detach().numpy(), label='p(Z)')\n",
    "    xj = xjs[j] #pxind.sample_n(n=1000)[j]\n",
    "    mu = irpm.latent_prior.log_partition.grad(irpm.rec_models[j](xj)+irpm.latent_prior.param)\n",
    "    m, sig2 = mu[:,0], mu[:,1] - mu[:,0]**2\n",
    "    N = len(m)\n",
    "    def fj(x):\n",
    "        return sum([torch.exp(torch.distributions.normal.Normal(loc=m[n], scale=torch.sqrt(sig2[n])).log_prob(x)) for n in range(N)])/N\n",
    "    plt.plot(xx.detach(), fj(xx).detach().numpy(), label='Fj(Z)')\n",
    "    plt.legend()\n",
    "    plt.title(r'prior $p(Z)$ vs factor evidence $F_j(Z)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [irpm, irpm]\n",
    "plt.figure(figsize=(7,7))\n",
    "i = 0\n",
    "logpx = models[i].eval([XX.reshape(-1,1), YY.reshape(-1,1)]).reshape(100,100).detach().numpy()\n",
    "plt.imshow(logpx, origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.contour(YY, XX , logpx, levels=5)\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "xjs = px.sample_n(n=50)\n",
    "plt.plot(xjs[1], xjs[0], 'r.')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('log p(x) under iRPM with full loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ce911",
   "metadata": {},
   "source": [
    "# test-bed playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian copula for data generation (rather than independence between marginals)\n",
    "import scipy.stats as stats\n",
    "A = np.random.normal(size=(J,J))\n",
    "P = A.dot(A.T)\n",
    "P = P / np.sqrt(np.outer(np.diag(P), np.diag(P)))\n",
    "N = 100000\n",
    "zz = stats.norm.cdf(A.dot(np.random.normal(size=(J,N))))\n",
    "xx = (-1.0/np.array(rates).reshape(J,1))*np.log(zz)\n",
    "\n",
    "\n",
    "for j in range(J):\n",
    "    plt.subplot(1,3,j+1)\n",
    "    plt.hist(xx[j,:], density=True)\n",
    "    plt.title(rates[j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ca94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
