{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ecea28",
   "metadata": {},
   "source": [
    "# Recognition-parametrized Variational autoencoders\n",
    "- $p_\\theta(\\mathcal{X},\\mathcal{Z})$ is a conditionally normalized RPM, whereas $q_\\psi(\\mathcal{Z} | \\mathcal{X})$ is from a jointly normalized RPM\n",
    "\n",
    "- all RPMS conditionally independent !\n",
    "\n",
    "- here application to to a 2D Gaussian copula with exponential marginals. $p_\\theta(\\mathcal{Z})$ and $f_{\\theta_j}(\\mathcal{Z}| \\bf{x}_j)$ are Gaussian for each $j =1, 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution, LogPartition_gauss_diagonal, LogPartition_vonMises\n",
    "from rpm import ExpFam, ConditionalExpFam, SemiparametricConditionalExpFam, RPM, RPVAE\n",
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "J = 2                           # two marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "dim_T = 2                       # dimension of sufficient statistics\n",
    "\n",
    "N = 1000\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'none'\n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "    xjs = px.sample_n(N)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "    pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "    xjs = px.sample_n(N)\n",
    "elif marginals == 'none':\n",
    "    Z = 1. * np.random.normal(size=(N, dim_Z))\n",
    "    def link(Z):\n",
    "        return np.stack([Z, np.tanh(Z)], axis=1)\n",
    "    xjs = [link(Z)[:,j] + 0.1 * np.random.normal(size=(N, dim_js[j])) for j in range(J)]\n",
    "    xjs = [torch.tensor(xj,dtype=dtype) for xj in xjs]\n",
    "else:\n",
    "    raise Exception('marginals not implemented')\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n",
    "\n",
    "\n",
    "# define Gaussian prior in natural parametrization\n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[...,:d], -torch.nn.Softplus()(x[...,d:])],axis=-1)\n",
    "log_partition = LogPartition_gauss_diagonal(d=dim_Z)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "latent_prior = ExpFam(natparam=torch.normal(mean=0.0, std=torch.ones(dim_T).reshape(1,-1)),\n",
    "                                    log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "natparam_models = [Net(dim_js[j], dim_T, n_hidden=50, activation_out=activation_out) for j in range(J)]\n",
    "rec_factors = [ConditionalExpFam(model=m, log_partition=log_partition) for m in natparam_models]\n",
    "\n",
    "ivi_natparam_models = [Net(sum(dim_js), dim_T, n_hidden=50, activation_out=activation_out) for j in range(J)]\n",
    "ivi_rec_models = [ConditionalExpFam(model=m, log_partition=log_partition) for m in ivi_natparam_models]\n",
    "\n",
    "#nu = torch.nn.parameter.Parameter(activation_out(torch.normal(mean=0.0, std=torch.ones(N, J, dim_T)/1000.)))\n",
    "nu = ivi_rec_models\n",
    "\n",
    "rpvae = RPVAE( rec_factors, latent_prior=latent_prior, px=pxjs, nu=nu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(rpvae.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(*xjs, torch.arange(N))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = rpvae.training_step(batch=batch[:-1], idx_data=batch[-1], batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = rpvae.training_step(batch=batch[:-1], idx_data=batch[-1], batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "    if J == 2:\n",
    "        XX,YY = torch.meshgrid(*xxs)\n",
    "        xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "        xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "    if J == 2:\n",
    "        XX,YY = torch.meshgrid(*xxs)\n",
    "        xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "        xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "    log_p0 = torch.log(torch.Tensor(rates)[0]) - rates[0] * xxs[0].reshape(-1,1) + torch.log(torch.Tensor(rates)[1]) - rates[1] * xxs[1].reshape(1,-1)\n",
    "elif marginals == 'none':\n",
    "    xxs = [torch.linspace(-3, 3,100), torch.linspace(-1.1, 1.1,100)]    \n",
    "    if J == 2:\n",
    "        XX,YY = torch.meshgrid(*xxs)\n",
    "        xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "        xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "    kdes = [KernelDensity(kernel=\"gaussian\", bandwidth='scott').fit(xjs[j]) for j in range(J)]\n",
    "    log_pj0s = [kdes[j].score_samples(xgrid[:,j]) for j in range(J)]\n",
    "    log_p0 = torch.tensor(sum([log_pj0s[j].reshape(100,100) for j in range(J)]), dtype=dtype)\n",
    "\n",
    "log_w = rpvae.elbo_innervi([xgrid[:,j] for j in range(J)]).reshape(100,100)\n",
    "#log_w = rpvae_test.elbo(xjs=[xgrid[:,j] for j in range(J)], idx_data=torch.arange(len(xgrid))).reshape(100,100)\n",
    "logpx = log_w + log_p0\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,3,1)\n",
    "try: \n",
    "    logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100).T\n",
    "    plt.imshow(np.exp(logpx_true), origin='lower', \n",
    "               extent=(xgrid[:,0].min(), xgrid[:,0].max(), xgrid[:,1].min(), xgrid[:,1].max()), aspect='auto')\n",
    "    #plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    #plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "    plt.colorbar()\n",
    "except:\n",
    "    plt.plot(xjs[0], xjs[1], '.')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.title('true p(x) and samples')\n",
    "    \n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()).T, origin='lower', \n",
    "           extent=(xgrid[:,0].min(), xgrid[:,0].max(), xgrid[:,1].min(), xgrid[:,1].max()), aspect='auto')\n",
    "#plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.title('p(x) of learned amortized RPM')\n",
    "#plt.plot(xjs[1], xjs[0], 'r.')\n",
    "try:\n",
    "    logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100).T\n",
    "    plt.colorbar()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "rec_factors, prior = rpvae.joint_model[0], rpvae.joint_model[1]\n",
    "eta0 = prior.nat_param\n",
    "phi0 = prior.phi()\n",
    "etajs_all = rpvae.factorNatParams(eta_off=eta0) # N-by-J-by-T\n",
    "phijs_all = torch.stack([rec_factors[j].log_partition(etajs_all[:,j]) for j in range(rpvae.J)],axis=1)\n",
    "\n",
    "Z = torch.linspace(-5, 5, 200)\n",
    "tZ = torch.stack([Z, Z**2], axis=1)\n",
    "pZ = torch.exp((eta0 * tZ).sum(axis=-1) - phi0)\n",
    "plt.plot(Z.detach().numpy(), pZ.detach().numpy(), label='prior p(Z)')\n",
    "\n",
    "for j in range(J):\n",
    "    etaj_all = etajs_all[:,j]\n",
    "    phij_all = phijs_all[:,j]\n",
    "    fj = torch.exp((etaj_all.unsqueeze(0) * tZ.unsqueeze(1)).sum(axis=-1) - phij_all.unsqueeze(0))\n",
    "    Fj = fj.mean(axis=1)\n",
    "    plt.plot(Z.detach().numpy(), Fj.detach().numpy(), label='Fj(Z), j='+str(j+1))\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('density')\n",
    "plt.title('Z-marginals')\n",
    "\n",
    "eta_q, eta_j = rpvae.comp_eta_q(xjs=[pxj.x for pxj in rpvae.joint_model[2].pxjs])\n",
    "phi_q = prior.log_partition(eta_q)\n",
    "q = torch.exp((eta_q.unsqueeze(0) * tZ.unsqueeze(1)).sum(axis=-1) - phi_q.unsqueeze(0))\n",
    "Q = q.mean(axis=1)\n",
    "plt.plot(Z.detach().numpy(), Q.detach().numpy(), ':', label='Q(Z)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc56dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for j in range(J):\n",
    "    etaj = rpvae.joint_model[0][j](xgrid[:,j]).detach().numpy()\n",
    "    sig2 = -2.0 / etaj[:,1]\n",
    "    mu = etaj[:,0]*sig2\n",
    "    plt.subplot(2,2,1+j*2)\n",
    "    plt.plot(xgrid[:,j].detach().numpy(), mu, '.')\n",
    "    if j == 0 :\n",
    "        plt.title(r'factor posterior mean $\\mu_j(x_j)$')\n",
    "    plt.ylabel('j = ' + str(j+1))\n",
    "    plt.subplot(2,2,2+j*2)\n",
    "    plt.plot(xgrid[:,j].detach().numpy(), sig2, '.')\n",
    "    if j == 0 :\n",
    "        plt.title(r'factor posterior variance $\\sigma_j^2(x_j)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4e713",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prior = rpvae.joint_model[1]\n",
    "eta_q, eta_j = rpvae.comp_eta_q(xjs)\n",
    "eta0, phi0 = prior.nat_param, rpvae.joint_model[1].phi()\n",
    "Z = torch.linspace(-20, 20, 2000)\n",
    "tZ = torch.stack([Z, Z**2], axis=1)\n",
    "pZ = torch.exp((eta0 * tZ).sum(axis=-1) - phi0)\n",
    "\n",
    "phi_q = prior.log_partition(eta_q)\n",
    "log_q = ((eta_q.unsqueeze(0) * tZ.unsqueeze(1) ).sum(axis=-1)- phi_q.unsqueeze(0))\n",
    "q = torch.exp(log_q)\n",
    "plt.plot(Z.detach().numpy(), q[:,:5].detach().numpy(), ':', label='Q(Z)')\n",
    "\n",
    "log_pZ = ((eta0.unsqueeze(0) * tZ ).sum(axis=-1)- prior.phi())[0]\n",
    "pZ = torch.exp(log_pZ)\n",
    "plt.plot(Z.detach().numpy(), pZ.detach().numpy())\n",
    "\n",
    "rec_factors = rpvae.joint_model[0]\n",
    "phijs = torch.stack([m.phi_x(xj, eta_off=eta0) for m,xj in zip(rec_factors,xjs)],axis=1)\n",
    "\n",
    "log_fjZ = ((eta_j.unsqueeze(0) * tZ.unsqueeze(1).unsqueeze(2) ).sum(axis=-1)- phijs.unsqueeze(0))\n",
    "fjZ =  torch.exp(log_fjZ)\n",
    "plt.plot(Z.detach().numpy(), fjZ[:,:5,0].detach().numpy(), '--')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print((fjZ.sum(axis=0) * torch.diff(Z)[0]).unique())\n",
    "\n",
    "\n",
    "log_q_ = (1-J) * log_pZ.unsqueeze(1) + log_fjZ.sum(axis=-1)\n",
    "q_ = torch.exp(log_q_)\n",
    "plt.plot(Z.detach().detach().numpy(), q_[:,:5].detach().numpy(), ':')\n",
    "plt.show()\n",
    "\n",
    "log_q_norm_num = torch.log(q_.sum(axis=0) * torch.diff(Z)[0])\n",
    "\n",
    "q_log_normalizer = rpvae.comp_q_log_normalizer(eta_q, eta_j, eta0)\n",
    "\n",
    "off = q_log_normalizer.max() - log_q_norm_num.max()\n",
    "\n",
    "plt.plot([log_q_norm_num.detach().numpy().min(), log_q_norm_num.detach().numpy().max()], \n",
    "         [log_q_norm_num.detach().numpy().min(), log_q_norm_num.detach().numpy().max()], \n",
    "         'k-', linewidth=0.5)\n",
    "plt.plot(log_q_norm_num.detach().numpy(), (- off + q_log_normalizer).detach().numpy(), '.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(phi_q.detach().numpy(), q_log_normalizer.detach().numpy(), '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d04f20",
   "metadata": {},
   "source": [
    "# VI RPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution, LogPartition_gauss_diagonal, LogPartition_vonMises\n",
    "from rpm import ExpFam, ConditionalExpFam, SemiparametricConditionalExpFam, RPM, RPVAE\n",
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "J = 2                           # two marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "dim_T = 2                       # dimension of sufficient statistics\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'none' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)] \n",
    "    pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "    xjs = px.sample_n(N)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "    pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "    xjs = px.sample_n(N)\n",
    "elif marginals == 'none':\n",
    "    Z = 2. * np.random.normal(size=(N, dim_Z))\n",
    "    def link(Z):\n",
    "        return np.stack([Z, np.tanh(Z)], axis=1)\n",
    "\n",
    "    xjs = [link(Z)[:,j] + 0.1 * np.random.normal(size=(N, dim_js[j])) for j in range(J)]\n",
    "    xjs = [torch.tensor(xj,dtype=dtype) for xj in xjs]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Gaussian prior in natural parametrization  \n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "log_partition = LogPartition_gauss_diagonal(d=dim_Z)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization  \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "latent_prior = ExpFam(natparam=torch.normal(mean=0.0, std=torch.ones(dim_T).reshape(1,-1)),\n",
    "                                    log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "natparam_models = [Net(dim_js[j], dim_T, n_hidden=50, activation_out=activation_out) for j in range(J)]\n",
    "rec_factors = [ConditionalExpFam(model=m, log_partition=log_partition) for m in natparam_models]\n",
    "\n",
    "q =  SemiparametricConditionalExpFam(natparams=torch.normal(mean=0.0, std=torch.ones(N, dim_T)), \n",
    "                                     log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "rpm = RPM( rec_factors, latent_prior=latent_prior, px=pxjs, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b094c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(rpm.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(*xjs, torch.arange(N))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = rpm.training_step(batch=batch[:-1], idx_data=batch[-1], batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = rpm.training_step(batch=batch[:-1], idx_data=batch[-1], batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_test =  SemiparametricConditionalExpFam(natparams=torch.normal(mean=0.0, std=torch.ones(len(xgrid), dim_T)), \n",
    "                                     log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "for p in rpm.joint_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "#optimizer = torch.optim.Adam(q_test.parameters(), lr=1e-3)\n",
    "q_train = rpm.q \n",
    "rpm.q = q_test\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(*[xgrid[:,j] for j in range(rpm.J)], torch.arange(len(xgrid)))\n",
    "dl_test = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl_test:\n",
    "        optimizer.zero_grad()\n",
    "        loss = rpm.training_step(batch=batch[:-1], idx_data=batch[-1], batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n",
    "\n",
    "for p in rpm.joint_model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpvae_ = RPVAE( rec_factors, latent_prior=latent_prior, px=pxjs, nu=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54980d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "    log_p0 = torch.log(torch.Tensor(rates)[0]) - rates[0] * xxs[0].reshape(-1,1) + torch.log(torch.Tensor(rates)[1]) - rates[1] * xxs[1].reshape(1,-1)\n",
    "elif marginals == 'none':\n",
    "    xxs = [torch.linspace(-3, 3,100), torch.linspace(-1.2, 1.2, 100)]    \n",
    "    kdes = [KernelDensity(kernel=\"gaussian\", bandwidth='scott').fit(xjs[j]) for j in range(J)]\n",
    "    log_pj0s = [kdes[j].score_samples(xgrid[:,j]) for j in range(J)]\n",
    "    log_p0 = torch.tensor(sum([log_pj0s[j].reshape(100,100) for j in range(J)]), dtype=dtype)\n",
    "if J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "\n",
    "log_w = rpvae_.elbo_innervi([xgrid[:,j] for j in range(J)]).reshape(100,100)\n",
    "#log_w = rpvae_test.elbo(xjs=[xgrid[:,j] for j in range(J)], idx_data=torch.arange(len(xgrid))).reshape(100,100)\n",
    "logpx = log_w + log_p0\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,3,1)\n",
    "try: \n",
    "    logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100).T\n",
    "    plt.imshow(np.exp(logpx_true), origin='lower', \n",
    "               extent=(xgrid[:,0].min(), xgrid[:,0].max(), xgrid[:,1].min(), xgrid[:,1].max()), aspect='auto')\n",
    "    #plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    #plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "    plt.colorbar()\n",
    "except:\n",
    "    plt.plot(xjs[0], xjs[1], '.')\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.title('true p(x) and samples')\n",
    "    \n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()).T, origin='lower', \n",
    "           extent=(xgrid[:,0].min(), xgrid[:,0].max(), xgrid[:,1].min(), xgrid[:,1].max()), aspect='auto')\n",
    "#plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.title('p(x) under dRPM')\n",
    "#plt.plot(xjs[1], xjs[0], 'r.')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "etajs_all = rpvae_.factorNatParams() # N-by-J-by-T\n",
    "rec_factors, prior = rpvae_.joint_model[0], rpvae_.joint_model[1]\n",
    "phijs_all = torch.stack([rec_factors[j].log_partition(etajs_all[:,j]) for j in range(rpvae_.J)],axis=1)\n",
    "eta0 = prior.nat_param\n",
    "phi0 = prior.phi()\n",
    "\n",
    "Z = torch.linspace(-0.5, 0.5, 200)\n",
    "tZ = torch.stack([Z, Z**2], axis=1)\n",
    "pZ = torch.exp((eta0 * tZ).sum(axis=-1) - phi0)\n",
    "plt.plot(Z.detach().numpy(), pZ.detach().numpy(), label='prior p(Z)')\n",
    "\n",
    "for j in range(J):\n",
    "    etaj_all = etajs_all[:,j]\n",
    "    phij_all = phijs_all[:,j]\n",
    "    fj = torch.exp((etaj_all.unsqueeze(0) * tZ.unsqueeze(1)).sum(axis=-1) - phij_all.unsqueeze(0))\n",
    "    Fj = fj.mean(axis=1)\n",
    "    plt.xlabel('Z')\n",
    "    plt.ylabel('Fj(Z)')\n",
    "    plt.plot(Z.detach().numpy(), Fj.detach().numpy(), label='Fj(Z), j='+str(j+1))\n",
    "\n",
    "eta_q, eta_j = rpvae_.comp_eta_q(xjs=[pxj.x for pxj in rpvae_.joint_model[2].pxjs])\n",
    "phi_q = prior.log_partition(eta_q)\n",
    "q = torch.exp((eta_q.unsqueeze(0) * tZ.unsqueeze(1)).sum(axis=-1) - phi_q.unsqueeze(0))\n",
    "Q = q.mean(axis=1)\n",
    "plt.plot(Z.detach().numpy(), Q.detach().numpy(), ':', label='Q(Z)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb39443",
   "metadata": {},
   "outputs": [],
   "source": [
    "etajs_all = rpm.factorNatParams()                     # N-by-J-by-T\n",
    "phijs_all = torch.stack([rpm.joint_model[0][j].log_partition(etajs_all[:,j]) for j in range(rpm.J)],axis=1)\n",
    "eta0 = rpm.joint_model[1].nat_param\n",
    "phi0 = rpm.joint_model[1].phi()\n",
    "\n",
    "Z = torch.linspace(-5, 5, 200)\n",
    "tZ = torch.stack([Z, Z**2], axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "pZ = torch.exp((eta0 * tZ).sum(axis=-1) - phi0)\n",
    "\n",
    "plt.plot(Z.detach().numpy(), pZ.detach().numpy(), label='prior p(Z)')\n",
    "for j in range(J):\n",
    "    etaj_all = etajs_all[:,j]\n",
    "    phij_all = phijs_all[:,j]\n",
    "    fj = torch.exp((etaj_all.unsqueeze(0) * tZ.unsqueeze(1)).sum(axis=-1) - phij_all.unsqueeze(0))\n",
    "    Fj = fj.mean(axis=1)\n",
    "    plt.xlabel('Z')\n",
    "    plt.ylabel('Fj(Z)')\n",
    "    plt.plot(Z.detach().numpy(), Fj.detach().numpy(), '--', label='Fj(Z), j='+str(j+1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8dfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
