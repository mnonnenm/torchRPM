{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e18a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "root = './data/MNIST'\n",
    "ds0train = torchvision.datasets.MNIST(root=root, train=True)\n",
    "ds0test = torchvision.datasets.MNIST(root=root, train=False)\n",
    "\n",
    "dtype=torch.float32\n",
    "\n",
    "def rotate(images):\n",
    "    batch_size = len(images)\n",
    "    angles= [0, 120, 240]\n",
    "    idx = np.random.randint(0, len(angles), batch_size)\n",
    "    with torch.no_grad(): \n",
    "        out = torch.stack([TF.rotate(images[i].unsqueeze(0), angles[idx[i]])[0] for i in range(batch_size)],dim=0)\n",
    "    return out\n",
    "\n",
    "def colorize(images):\n",
    "    batch_size = len(images)\n",
    "    colors = np.array(\n",
    "        [[166,206,227],\n",
    "         [31,120,180],\n",
    "         [178,223,138],\n",
    "         [51,160,44],\n",
    "         [251,154,153]],\n",
    "    )/256.\n",
    "    colors = np.asarray(colors,dtype=np.float32)\n",
    "    idx = np.random.randint(0, len(colors), batch_size)\n",
    "    with torch.no_grad():\n",
    "        out=torch.stack([(images[i].unsqueeze(0).repeat(3,1,1)*colors[idx[i]].reshape(-1,1,1)) for i in range(batch_size)],dim=0) \n",
    "\n",
    "    return out\n",
    "\n",
    "transforms_j = rotate, colorize\n",
    "\n",
    "class Peersupervision(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, J, transforms, ifstack=True):\n",
    "        self.data = data\n",
    "        self.targets = targets.detach().numpy()\n",
    "        self.J = J\n",
    "        self.ifstack = ifstack\n",
    "        assert len(transforms) == J\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        c = self.targets[idx]\n",
    "        if c.ndim==0:\n",
    "            idc = np.where(self.targets==c)[0]\n",
    "            pair_ids = idc[np.random.choice(len(idc), self.J+1, replace=False).reshape(1,-1)]\n",
    "            pair_ids[0,0] = idx\n",
    "        else:\n",
    "            pair_ids = np.zeros((len(idx), self.J))\n",
    "            for i,c_ in enumerate(c):\n",
    "                idc = np.where(self.targets==c_)[0]\n",
    "                pair_ids[i] = idc[np.random.choice(len(idc), self.J, replace=False)]\n",
    "            pair_ids[:,0] = idx\n",
    "\n",
    "        out = [self.transforms[j](self.data[pair_ids[:,j]]) for j in range(self.J)]\n",
    "        return (torch.stack(out,axis=1), idx) if self.ifstack else (out, idx) \n",
    "\n",
    "N,J = len(ds0train), 2\n",
    "\n",
    "ds_train = Peersupervision(data=ds0train.data/256., targets=ds0train.targets, J=J, transforms=transforms_j,ifstack=False)\n",
    "train_data = ds_train[np.arange(N)]\n",
    "train_data = [[train_data[0][0].unsqueeze(1), train_data[0][1]], train_data[1]]\n",
    "train_labels = ds0train.targets\n",
    "\n",
    "ds_test = Peersupervision(data=ds0test.data/256., targets=ds0test.targets, J=J, transforms=transforms_j, ifstack=False)\n",
    "test_data = ds_test[np.arange(len(ds0test))]\n",
    "test_data = [[test_data[0][0].unsqueeze(1), test_data[0][1]], test_data[1]]\n",
    "test_labels = ds0test.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,6))\n",
    "for n in range(np.minimum(N, 5)):\n",
    "    for j in range(J):\n",
    "        plt.subplot(np.minimum(N, 5), J, J*n + j + 1)\n",
    "        data_show = train_data[0][n,j].detach().numpy() if ds_train.ifstack else train_data[0][j][n].detach().numpy()\n",
    "        if j == 0:\n",
    "            plt.imshow(data_show[0], cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(data_show.transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "plt.suptitle('First ' + str(np.minimum(N, 5)) + ' out of N= ' +str(N) + ' peer tuples of size J =' +str(J))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c42d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution, RPM\n",
    "from expFam import LogPartition_gauss_diagonal, ExpFam, ConditionalExpFam\n",
    "\n",
    "\n",
    "K = len(np.unique(train_labels.detach().numpy()))\n",
    "#dim_T = K # dimension of sufficient statistics\n",
    "dim_T = 4 # dimension of sufficient statistics\n",
    "dim_Z = 2 # dimension of sufficient statistics\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    # Convolutional Neural Network shared across independent factors\n",
    "    def __init__(self, C_in, n_out, C_hidden, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.conv1 = torch.nn.Conv2d(C_in, C_hidden, kernel_size=5)\n",
    "        self.conv2 =torch.nn.Conv2d(C_hidden, 2*C_hidden, kernel_size=5)\n",
    "        self.conv2_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(4*4*2*C_hidden, n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*4*20)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "# define Gaussian prior in natural parametrization\n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[...,:d], -torch.nn.Softplus()(x[...,d:])],axis=-1)\n",
    "log_partition = LogPartition_gauss_diagonal(d=dim_Z)\n",
    "\n",
    "latent_prior = ExpFam(natparam=torch.normal(mean=0.0, std=torch.ones(dim_T).reshape(1,-1)),\n",
    "                                    log_partition=log_partition, activation_out=activation_out)\n",
    "\n",
    "C_ins_j = [1,3] # number of input channels (per j) for CNNs\n",
    "natparam_models = [Net(C_in=C_ins_j[j], \n",
    "                       n_out=dim_T, \n",
    "                       C_hidden=10, \n",
    "                       n_hidden=50, \n",
    "                       activation_out=activation_out) for j in range(J)]\n",
    "rec_factors = [ConditionalExpFam(model=natparam_models[j], log_partition=log_partition) for j in range(J)]\n",
    "\n",
    "ivi_natparam_models = [Net(C_in=sum(C_ins_j), n_out=dim_T, C_hidden=10, n_hidden=50, activation_out=activation_out) for j in range(J)]\n",
    "ivi_rec_models = [ConditionalExpFam(model=m, log_partition=log_partition) for m in ivi_natparam_models]\n",
    "\n",
    "nu = ivi_rec_models\n",
    "\n",
    "xjs = [train_data[0][j] for j in range(J)]\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n",
    "\n",
    "rpm = RPM(rec_factors, latent_prior=latent_prior, px=pxjs, \n",
    "          q='use_theta',  \n",
    "          nu=nu, iviNatParametrization='delta',\n",
    "          stack_xjs_new_axis=False, full_N_for_Fj=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6174f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(rpm.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "ds_load = torch.utils.data.TensorDataset(*train_data[0], torch.tensor(train_data[1]))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds_load, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch[0] if ds_train.ifstack else ((batch[0], batch[1]), batch[2])\n",
    "        loss = rpm.training_step(*batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "        #print(t)\n",
    "    print('epoch #' + str(i+1) + '/' + str(epochs) + ', loss : ' + str(ls[t-1]))\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_q = rpm.comp_eta_q(xjs)[0]\n",
    "mu_q = log_partition.nat2meanparam(eta_q)\n",
    "for k in range(10):    \n",
    "    plt.subplot(2,5,k+1)\n",
    "    plt.imshow(np.histogram2d(mu_q[train_labels==k, 0].detach().numpy(), \n",
    "                              mu_q[train_labels==k, 1].detach().numpy())[0])\n",
    "    #plt.xlim(mu_q[:,0].detach().min(), mu_q[:,0].detach().max())\n",
    "    #plt.ylim(mu_q[:,1].detach().min(), mu_q[:,1].detach().max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dim_Z):\n",
    "    for k in range(10):    \n",
    "        plt.subplot(10,2,i+2*k+1)\n",
    "        plt.hist(mu_q[train_labels==k, i].detach().numpy())\n",
    "        plt.xlim(mu_q[:,i].detach().min(), mu_q[:,i].detach().max())\n",
    "        #plt.ylim(mu_q[:,1].detach().min(), mu_q[:,1].detach().max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch[0] if ds_train.ifstack else ((batch[0], batch[1]), batch[2])\n",
    "        loss = rpm.training_step(*batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "        #print(t)\n",
    "    print('epoch #' + str(i+1) + '/' + str(epochs) + ', loss : ' + str(ls[t-1]))\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2339d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_q = rpm.comp_eta_q(xjs)[0]\n",
    "mu_q = log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "for k in range(10):\n",
    "    plt.subplot(10,1,k+1)\n",
    "    plt.hist(mu_q[train_labels==k, 0].detach().numpy())\n",
    "    plt.xlim(mu_q[:,0].detach().min(), mu_q[:,0].detach().max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f650c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "xjs = [train_data[0][j] for j in range(J)]\n",
    "idx_n = train_data[1].reshape(-1,1)\n",
    "\n",
    "log_pzj_xs, log_pzg_x, log_px = drpm.eval(xjs, idx_n)\n",
    "posts = log_pzg_x.detach().numpy() # posteriors over global (!) latent\n",
    "\n",
    "M = skmetrics.confusion_matrix(y_true=np.argmax(posts,axis=1), y_pred=train_labels)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapij(a):\n",
    "    b = np.zeros_like(a)\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == 0:\n",
    "            b[i] =   2\n",
    "        if a[i] == 1:\n",
    "            b[i] =   6\n",
    "        if a[i] == 2:\n",
    "            b[i] =   1\n",
    "        if a[i] == 3:\n",
    "            b[i] =   8\n",
    "        if a[i] == 4:\n",
    "            b[i] =   4\n",
    "        if a[i] == 5:\n",
    "            b[i] =   3\n",
    "        if a[i] == 6:\n",
    "            b[i] =   5\n",
    "        if a[i] == 7:\n",
    "            b[i] =   0\n",
    "        if a[i] == 8:\n",
    "            b[i] =   9\n",
    "        if a[i] == 9:\n",
    "            b[i] =   7\n",
    "    return b\n",
    "\n",
    "Mperm = skmetrics.confusion_matrix(y_true=mapij(np.argmax(posts,axis=1)), y_pred=train_labels)\n",
    "plt.imshow(Mperm)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "np.diag(Mperm).sum() / Mperm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37381b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "xjs = [test_data[0][j] for j in range(J)]\n",
    "idx_n = test_data[1].reshape(-1,1)\n",
    "log_pzj_xs, log_pzg_x, log_px = drpm.eval(xjs, idx_n)\n",
    "posts = log_pzg_x.detach().numpy() # posteriors over global (!) latent\n",
    "\n",
    "M = skmetrics.confusion_matrix(y_true=np.argmax(posts,axis=1), y_pred=test_labels)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "Mperm = skmetrics.confusion_matrix(y_true=mapij(np.argmax(posts,axis=1)), y_pred=test_labels)\n",
    "plt.imshow(Mperm)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "np.diag(Mperm).sum() / Mperm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28d30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
