{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "root = './data/MNIST'\n",
    "ds0train = torchvision.datasets.MNIST(root=root, train=True)\n",
    "ds0test = torchvision.datasets.MNIST(root=root, train=False)\n",
    "dtype=torch.float32\n",
    "\n",
    "\n",
    "class Peersupervision(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, J, ifstack=True):\n",
    "        self.data = data\n",
    "        self.targets = targets.detach().numpy()\n",
    "        self.J = J\n",
    "        self.ifstack = ifstack\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        c = self.targets[idx]\n",
    "        if c.ndim==0:\n",
    "            idc = np.where(self.targets==c)[0]\n",
    "            pair_ids = idc[np.random.choice(len(idc), self.J+1, replace=False).reshape(1,-1)]\n",
    "            pair_ids[0,0] = idx\n",
    "        else:\n",
    "            pair_ids = np.zeros((len(idx), self.J))\n",
    "            for i,c_ in enumerate(c):\n",
    "                idc = np.where(self.targets==c_)[0]\n",
    "                pair_ids[i] = idc[np.random.choice(len(idc), self.J, replace=False)]\n",
    "            pair_ids[:,0] = idx\n",
    "\n",
    "        out = [self.data[pair_ids[:,j]] for j in range(self.J)]\n",
    "        return (torch.stack(out,axis=1), idx) if self.ifstack else (out, idx) \n",
    "\n",
    "N,J = len(ds0train), 2\n",
    "\n",
    "ds_train = Peersupervision(data=ds0train.data/256., targets=ds0train.targets, J=J, ifstack=False)\n",
    "train_data = ds_train[np.arange(N)]\n",
    "train_data = [[train_data[0][0].unsqueeze(1), train_data[0][1].unsqueeze(1)], train_data[1]]\n",
    "train_labels = ds0train.targets\n",
    "\n",
    "ds_test = Peersupervision(data=ds0test.data/256., targets=ds0test.targets, J=J, ifstack=False)\n",
    "test_data = ds_test[np.arange(len(ds0test))]\n",
    "test_data = [[test_data[0][0].unsqueeze(1), test_data[0][1].unsqueeze(1)], test_data[1]]\n",
    "test_labels = ds0test.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752240f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "#fracs_k = [5/600, 5/600, 5/600, 5/600, 5/600, 5/600, 10/600, 10/600, 25/600, 25/600]\n",
    "fracs_k = [10/600, 10/600, 10/600, 10/600, 10/600, 10/600, 10/600, 10/600, 10/600, 10/600]\n",
    "idx_ks = []\n",
    "for k in range(K):\n",
    "    idx_k = np.where(train_labels == k)[0]\n",
    "    idx_k = idx_k[:np.int32(np.round(len(idx_k)*fracs_k[k]))]\n",
    "    idx_ks.append(idx_k)\n",
    "idx_re = np.concatenate(idx_ks)\n",
    "\n",
    "N = len(idx_re)\n",
    "\n",
    "train_labels = train_labels[idx_re]\n",
    "train_data = [[train_data[0][0][idx_re], train_data[0][1][idx_re]], np.arange(N)]\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6431e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,6))\n",
    "for n in range(np.minimum(N, 5)):\n",
    "    for j in range(J):\n",
    "        plt.subplot(np.minimum(N, 5), J, J*n + j + 1)\n",
    "        data_show = train_data[0][n,j].detach().numpy() if ds_train.ifstack else train_data[0][j][n].detach().numpy()\n",
    "        plt.imshow(data_show[0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "plt.suptitle('First ' + str(np.minimum(N, 5)) + ' out of N= ' +str(N) + ' peer tuples of size J =' +str(J))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution\n",
    "from discreteRPM import discreteRPM, Prior_discrete, discreteRPM_softmaxForm\n",
    "from implicitRPM import ObservedMarginal, IndependentMarginal\n",
    "from discreteRPM import RecognitionFactor_discrete, RecognitionFunction_discrete, RecognitionFunction_discrete_norm\n",
    "\n",
    "K = len(np.unique(train_labels.detach().numpy()))\n",
    "dim_T = K # dimension of sufficient statistics\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    # Convolutional Neural Network shared across independent factors\n",
    "    def __init__(self, C_in, n_out, C_hidden, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.conv1 = torch.nn.Conv2d(C_in, C_hidden, kernel_size=5)\n",
    "        self.conv2 =torch.nn.Conv2d(C_hidden, 2*C_hidden, kernel_size=5)\n",
    "        self.conv2_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(4*4*2*C_hidden, n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*4*20)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "natparam_models = [Net(C_in=1, \n",
    "                       n_out=K+1, # K+1 for the extra term hj(xj) !\n",
    "                       C_hidden=10, \n",
    "                       n_hidden=50, \n",
    "                       activation_out=torch.nn.Identity()) for j in range(J)]\n",
    "#rec_models = [RecognitionFunction_discrete(model=natparam_models[j]) for j in range(J)]\n",
    "rec_models = [RecognitionFunction_discrete_norm(model=natparam_models[j]) for j in range(J)]\n",
    "\n",
    "\n",
    "prior =  Prior_discrete(param=torch.zeros(size=(K,)))\n",
    "\n",
    "xjs = [train_data[0][j] for j in range(J)]\n",
    "pxjs = [EmpiricalDistribution(xjs[j]) for j in range(J)]\n",
    "px_allj = RPMEmpiricalMarginals(xjs)\n",
    "\n",
    "# constsruct implicit RPM\n",
    "drpm = discreteRPM_softmaxForm(rec_models, latent_prior=prior, pxjs=pxjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "xjs = [train_data[0][j].unsqueeze(1) for j in range(J)]\n",
    "idx_n = torch.tensor(train_data[1]).unsqueeze(1)\n",
    "\n",
    "log_w, posterior = drpm.eval(idx_n)\n",
    "posts = posterior.detach().numpy() # posteriors over global (!) latent\n",
    "\n",
    "M = skmetrics.confusion_matrix(y_true=np.argmax(posts,axis=1), y_pred=train_labels)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "xjis = [drpm.pxjs[j].x[:10000] for j in range(drpm.J)]                                # N - D\n",
    "gji = [m.affine_all_z(xj) for m,xj in zip(drpm.rec_models, xjis)]             # N - K  x J\n",
    "norms_init = torch.exp(gji[0]).sum(axis=1).detach().numpy()\n",
    "plt.plot(np.sort(norms_init))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7624a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(drpm.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "#ds_load = torch.utils.data.TensorDataset(*test_data[0], torch.tensor(test_data[1]))\n",
    "ds_load = torch.utils.data.TensorDataset(*train_data[0], torch.tensor(train_data[1]))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds_load, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        idx_n = batch[2]\n",
    "        loss = drpm.training_step(idx_n, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "    print('epoch #' + str(i+1) + '/' + str(epochs) + ', loss : ' + str(ls[t-1]))\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "xjs = [train_data[0][j].unsqueeze(1) for j in range(J)]\n",
    "idx_n = torch.tensor(train_data[1]).unsqueeze(1)\n",
    "\n",
    "log_w, posterior = drpm.eval(idx_n)\n",
    "posts = posterior.detach().numpy() # posteriors over global (!) latent\n",
    "\n",
    "M = skmetrics.confusion_matrix(y_true=np.argmax(posts,axis=1), y_pred=train_labels)\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "xjis = [drpm.pxjs[j].x[:10000] for j in range(drpm.J)]                                # N - D\n",
    "gji = [m.affine_all_z(xj) for m,xj in zip(drpm.rec_models, xjis)]             # N - K  x J\n",
    "norms_early = torch.exp(gji[0]).sum(axis=1).detach().numpy()\n",
    "plt.plot(np.sort(norms_early))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_js = []\n",
    "for k in range(K):\n",
    "    idx_js.append(np.where(np.argmax(posts,axis=1) == k)[0])\n",
    "idx_re = np.concatenate(idx_js)\n",
    "train_images_re = [train_data[0][0][idx_re], train_data[0][1][idx_re]]\n",
    "\n",
    "idx_n = idx_re #np.arange(N)\n",
    "    \n",
    "gji = [m.affine_all_z(xj[idx_n]) for m,xj in zip(drpm.rec_models, xjis)]       # N - K  x J\n",
    "log_Zj = [torch.logsumexp(gji[j],axis=0).unsqueeze(0) for j in range(drpm.J)]  # 1 - K  x J  \n",
    "log_aji = [gji[j] - log_Zj[j] for j in range(drpm.J) ]                         # b - K  x J\n",
    "log_aji = [log_aji[j] - torch.log(torch.exp(log_aji[j]).sum(axis=-1)).unsqueeze(-1) for j in range(drpm.J)]\n",
    "\n",
    "log_joint = drpm.latent_prior.log_probs() + (log_aji[0].unsqueeze(0)+log_aji[1].unsqueeze(1))\n",
    "marginal = torch.exp(log_joint).sum(axis=-1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for j in range(J):\n",
    "    plt.subplot(1,2,j+1)\n",
    "    if j == 0:\n",
    "        plt.ylabel('kj')\n",
    "    plt.imshow((torch.exp(log_aji[j])/torch.exp(log_aji[j]).sum(axis=0).unsqueeze(0)).detach().numpy(), \n",
    "                aspect='auto', interpolation='none')\n",
    "    plt.xlabel('Z')\n",
    "    plt.colorbar()\n",
    "plt.suptitle('p(kj | Z) for j = 1, 2')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(marginal.detach().numpy())\n",
    "plt.title('marginal P(K) on standard grid after permuting indices')\n",
    "plt.show()\n",
    "#log_normalizer = torch.exp(log_joint).sum(axis=-1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.diag(marginal.detach().numpy()), label='p(K=[n,n,..])')\n",
    "plt.plot(np.arange(K)*N/K+0.5*N/K, torch.exp(drpm.latent_prior.log_probs()).detach().numpy(), 'o-', label='prior p(Z=k)')\n",
    "plt.legend()\n",
    "plt.title('diagonal profile of P(K)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(torch.exp(drpm.latent_prior.log_probs()).detach().numpy(), np.array([len(idx) for idx in idx_js]), 'x')\n",
    "plt.title('prior p(Z) vs cluster size')\n",
    "plt.xlabel('p(Z)')\n",
    "plt.ylabel('#n which argmax_kj p(kj|Z) = n')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f2567",
   "metadata": {},
   "source": [
    "# skewed label distributions \n",
    "- 25% of data are 8's\n",
    "- 25% of data are 9's\n",
    "- remaining 8 classes each constitute 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewed label distributions - 25% of data are 8's, 25% of data are 9's, remaining 8 classes each constitute 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(norms_init),  label='init')\n",
    "plt.plot(np.sort(norms_early), label='early')\n",
    "plt.plot(np.sort(norms_late),  label='late')\n",
    "plt.plot(np.sort(norms_final),  label='final')\n",
    "plt.plot([0, len(norms_init)], [1, 1], 'k:')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e74eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31024d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
