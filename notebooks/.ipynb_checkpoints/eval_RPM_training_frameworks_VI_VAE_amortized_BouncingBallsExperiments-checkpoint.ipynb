{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cce20c5",
   "metadata": {},
   "source": [
    "# training frameworks for the RPM on continous latents\n",
    "\n",
    "Consider a recognition-parametrized model\n",
    "$$\n",
    "p_\\theta(\\mathcal{X},\\mathcal{Z}) = p_\\theta(\\mathcal{Z}) \\prod_j \\frac{f_{\\theta_j}(\\mathcal{Z}|{x}_j) p_j(x_j)}{F_{\\theta_j}(\\mathcal{Z})}\n",
    "$$\n",
    "with exponential family $f_{\\theta_j}(\\mathcal{Z}|x_j), p(\\theta(\\mathcal{Z})$ trained via (a lower bound to) variational free energy\n",
    "\\begin{align}\n",
    "    \\log p_\\theta(\\mathcal{X}) &\\geq \\mathbb{E}_{q(\\mathcal{Z}|\\mathcal{X})}\\left[ \\log \\frac{p_\\theta(\\mathcal{X}, \\mathcal{Z})}{q_\\psi(\\mathcal{Z}|\\mathcal{X})}\\right] \\nonumber \\\\\n",
    " &= \\mathbb{E}_{q}[\\log p_\\theta(\\mathcal{Z})] + \\sum_j \\mathbb{E}_{q}[\\log f_{\\theta_j}(\\mathcal{Z}|{x}_j) ] + H[q] - \\sum_j \\mathbb{E}_q[\\log F_{\\theta_j}(\\mathcal{Z})] + const. \\nonumber \\\\    \n",
    " &\\geq \\mathbb{E}_{q}[\\log p_\\theta(\\mathcal{Z})] + \\sum_j \\mathbb{E}_{q}[\\log f_{\\theta_j}(\\mathcal{Z}|{x}_j) ] - (1-J) H[q] - \\sum_j \\log \\int F_{\\theta_j}(\\mathcal{Z})/h_j(\\mathcal{Z}) d\\mathcal{Z} - \\mathbb{E}_q[\\log h_j(\\mathcal{Z})] + const. \\nonumber \n",
    "\\end{align}\n",
    "where $h_j(\\mathcal{Z}) = \\exp(\\tilde{\\eta}_j(\\mathcal{X})^\\top{}t(\\mathcal{Z}))$ with inner variational parameters $\\tilde{\\eta}_j$ (one per data-point $\\mathcal{X}$).\n",
    "\n",
    "There are several ways we can handle the recognition model $q(\\mathcal{Z}|\\mathcal{X})$, even if for all of them we assume $q(\\mathcal{Z}|\\mathcal{X}) = q(\\mathcal{Z} | \\eta_q(\\mathcal{X}))$ to lie in the same exponential family as $f_{\\theta_j}(\\mathcal{Z}|x_j), p_\\theta(\\mathcal{Z})$:\n",
    "- $\\eta_q(\\mathcal{X}^{(n)})=\\eta_q^{(n)}$: nonparametric natural parameter model (VI) with own natural parameter per datum $n$.\n",
    "- $\\eta_q(\\mathcal{X}^{(n)})= NN_\\psi(\\mathcal{X}^{(n)})$ : parametric natural parameter model (VAE)  with recognition parameters $\\psi$.\n",
    "- $\\eta_q(\\mathcal{X}^{(n)}) = \\sum_j \\eta_{\\theta_j}(x_j^{(n)}) + (1-J) \\eta_0$ : analytic approach, holds if we assume $\\forall j: F_{\\theta_j}(\\mathcal{Z}) = p_\\theta(\\mathcal{Z})$.\n",
    "\n",
    "Similarly, we can handle inner variational parameters $\\tilde{\\eta}_j(\\mathcal{X})$ as \n",
    "- $\\forall j: \\tilde{\\eta}_j(\\mathcal{X}^{(n)})=\\tilde{\\eta}_j^{(n)}$ : nonparametric.  \n",
    "- $\\forall j:\\tilde{\\eta}_{j}(\\mathcal{X}^{(n)})= NN_{\\psi_j}(\\mathcal{X}^{(n)})$ : parametric with parameters $\\{\\psi_j\\}_j$.\n",
    "- $\\forall j:\\tilde{\\eta}_{j}(\\mathcal{X}^{(n)}) = \\eta_0 - \\eta_q(\\mathcal{X}^{(n)})$ : Hugo's Ansatz with whatever choice for $\\eta_q(\\mathcal{X})$ we took above.\n",
    "\n",
    "This gives us in total some $3 \\times 3 = 9$ combinations on how to train the very same RPM $p_\\theta(\\mathcal{X},\\mathcal{Z})$, which we will explore in the remainder. \n",
    "\n",
    "For experiments, we'll here focus on some with temporal structure in the latents. This motivates a time-series version of the RPM above, \n",
    "$$\n",
    "p_\\theta(\\mathcal{X},\\mathcal{Z}) = p_\\theta(\\mathcal{Z}) \\prod_t \\prod_j \\frac{f_{\\theta_j}(\\mathcal{Z}_t|{x}_{jt}) p_{jt}(x_{jt})}{F_{\\theta_j}(\\mathcal{Z}_t)},\n",
    "$$\n",
    "which we will also test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037462fb",
   "metadata": {},
   "source": [
    "# comparing number of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59578842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "from utils_setup import init_gaussian_rpm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c226cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [ [[102,194,164],\n",
    "            [44,162,95],     # VI: greens\n",
    "            [0,109,44]\n",
    "           ],\n",
    "           [[116,169,207],\n",
    "            [43,140,190],    # VAE: blues\n",
    "            [4,90,141]\n",
    "           ],\n",
    "          [[254,153,41],\n",
    "           [217,95,14],      # Amortized: oranges\n",
    "           [153,52,4]              \n",
    "          ]\n",
    "         ]\n",
    "\n",
    "colors = np.array(colors)/256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "J, K, T = 10, 1, 50\n",
    "\n",
    "init_rb_bandwidth = 1000.\n",
    "obs_locs = torch.linspace(0, 1, T)\n",
    "\n",
    "rpm_variants = ['VI', 'VAE', 'amortized']\n",
    "amortize_ivis = ['none', 'full', 'use_q']\n",
    "\n",
    "temporal = True\n",
    "\n",
    "Ns = [10, 100, 1000]\n",
    "\n",
    "numels = np.zeros((len(Ns), len(rpm_variants), len(amortize_ivis)))\n",
    "\n",
    "\n",
    "for n,N in enumerate(Ns):\n",
    "    \n",
    "    # placeholder data\n",
    "    xjs = [torch.zeros((N,T)) for j in range(J)]\n",
    "    pxjs = RPMEmpiricalMarginals(xjs)\n",
    "    \n",
    "    for i,rpm_variant in enumerate(rpm_variants):\n",
    "        for j,amortize_ivi in enumerate(amortize_ivis):\n",
    "\n",
    "            # init model\n",
    "            model = init_gaussian_rpm(\n",
    "                N, J, K, T, pxjs,\n",
    "                init_rb_bandwidth, obs_locs,\n",
    "                rpm_variant, temporal, amortize_ivi,\n",
    "                epochs=0, batch_size=N,\n",
    "                dim_T=2, n_hidden=20, \n",
    "                optim_init_q=False, optim_init_ivi=False, optim_vae_params=False)[0]\n",
    "\n",
    "            # count parameters\n",
    "            numels[n,i,j] = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for n in range(len(Ns)):\n",
    "    print(numels[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbaf2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for n in range(len(Ns)):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(len(amortize_ivis)):\n",
    "        ax = plt.subplot(3,1,i+1)\n",
    "        max_y_lim = numels[n].max()\n",
    "        plt.ylim(0, max_y_lim)\n",
    "        plt.bar(x=rpm_variants, height=numels[n,:,i], color=[colors[j,i] for j in range(len(rpm_variants))])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.xaxis.tick_top()\n",
    "        plt.yticks([0, 500*Ns[n], 1000*Ns[n]])\n",
    "        valignm = 'top' if i==0 else 'bottom'\n",
    "        for j in range(len(rpm_variants)):\n",
    "            plt.text(j, numels[n,j,i], str(int(numels[n,j,i])),\n",
    "                     horizontalalignment='center', verticalalignment=valignm)\n",
    "        if not i == 0:\n",
    "            plt.xticks([])\n",
    "        plt.ylabel(amortize_ivis[i])\n",
    "    plt.suptitle(r'total parameter counts ($\\theta, \\psi, \\{\\tilde{\\eta}_{j}^{(n)}\\}$), for N = ' +str(Ns[n]))\n",
    "    plt.savefig('figs/paramCounts_N' + str(Ns[n]) + '.pdf', bbox_inches ='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897956b",
   "metadata": {},
   "source": [
    "# comparing initializations by loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b606d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_data_external import linear_regression_1D_latent as regLatent\n",
    "from utils_data_external import plot_poisson_balls\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_setup import init_gaussian_rpm\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "\n",
    "N = 100\n",
    "temporal = True\n",
    "\n",
    "num_steps = 5000 if N==10 else 12000\n",
    "\n",
    "rpm_variants = ['VAE', 'VI', 'amortized']\n",
    "amortize_ivis = ['none', 'full', 'use_q']\n",
    "model_seeds = [0]\n",
    "\n",
    "inits = ['fits_reparam', 'fits_pretrainAsQ'] #['fits_noPretrain', 'fits_pretrainElbo', 'fits_pretrainAsQ'] #, 'fits_pretrainElbo', 'fits_pretrainAsQ']\n",
    "obs_locs = torch.linspace(0,1,T).reshape(-1,1)\n",
    "\n",
    "losses_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), num_steps))\n",
    "losses_test = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), num_steps))\n",
    "\n",
    "latents_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), N, 50))\n",
    "true_latents_train = np.zeros((len(model_seeds), N, 50))\n",
    "data_train_show  = np.zeros((len(model_seeds), N, J, T))\n",
    "\n",
    "for s,model_seed in enumerate(model_seeds):\n",
    "    for r,res_dir in enumerate(inits):\n",
    "        for i,rpm_variant in enumerate(rpm_variants):\n",
    "            for j,amortize_ivi in enumerate(amortize_ivis):\n",
    "\n",
    "                if temporal:\n",
    "                    identifier = rpm_variant + '_temp_' + amortize_ivi\n",
    "                else:\n",
    "                    identifier = rpm_variant + '_' + amortize_ivi        \n",
    "                identifier = identifier + '_N_' + str(N) + '_seed_' + str(model_seed)\n",
    "\n",
    "                root = os.curdir\n",
    "                fn_base = os.path.join(res_dir, identifier, identifier)\n",
    "\n",
    "                data = torch.tensor(np.load(fn_base + '_train_data.npy'))\n",
    "                true_latent_ext = torch.tensor(np.load(fn_base + '_train_latents.npy'))\n",
    "                #data = torch.tensor(np.load(fn_base + '_test_data.npy'))\n",
    "                #true_latent_ext = torch.tensor(np.load(fn_base + '_test_latents.npy'))\n",
    "\n",
    "                exp_dict = np.load(fn_base + '_exp_dict.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "                N,J,K,T = exp_dict['N'],exp_dict['J'],exp_dict['K'],exp_dict['T']\n",
    "                #init_diag_val = exp_dict['init_diag_val']\n",
    "                #init_off_val = exp_dict['init_off_val']\n",
    "                init_rb_bandwidth = exp_dict['init_rb_bandwidth']\n",
    "\n",
    "                ls_train = np.load(fn_base + '_loss_train.npy')\n",
    "                ls_test = np.load(fn_base + '_loss_test.npy')\n",
    "\n",
    "                xjs = [data[:,j] for j in range(J)]\n",
    "                pxjs = RPMEmpiricalMarginals(xjs)\n",
    "                observations = (torch.stack(xjs, dim=-1),)\n",
    "\n",
    "\n",
    "                model = init_gaussian_rpm(N, J, K, T, pxjs,\n",
    "                                        init_rb_bandwidth, obs_locs,\n",
    "                                        rpm_variant, temporal, amortize_ivi,\n",
    "                                        epochs=0, batch_size=N\n",
    "                                       )[0]\n",
    "                try:\n",
    "                    model.load_state_dict(torch.load(fn_base + '_rpm_state_dict.zip'))\n",
    "                except:\n",
    "                    model.load_state_dict(torch.load(fn_base + '_rpm_state_dict'))\n",
    "\n",
    "\n",
    "                prior = model.joint_model[1]\n",
    "                eta_0 = prior.nat_param\n",
    "                if rpm_variant == 'amortized':    \n",
    "                    eta_q, _ = model.comp_eta_q(xjs, eta_0)\n",
    "                else: \n",
    "                    eta_q = model.comp_eta_q(xjs, eta_0=eta_0, idx_data=np.arange(N))\n",
    "                EqtZ = prior.log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "                mu = EqtZ[:,:T]\n",
    "                sig2 = torch.diagonal(EqtZ[:,T:].reshape(-1,T,T),dim1=-2,dim2=-1) - mu**2\n",
    "\n",
    "                latent_true, latent_mean_fit, latent_variance_fit, R2 = regLatent(\n",
    "                    latent_true = true_latent_ext,\n",
    "                    latent_mean_fit = mu.unsqueeze(-1), \n",
    "                    latent_variance_fit = sig2)\n",
    "\n",
    "                #plot_poisson_balls(observations, \n",
    "                #                   obs_locs=obs_locs.squeeze(-1), \n",
    "                #                   latent_mean_fit=latent_mean_fit.squeeze(-1), \n",
    "                #                   latent_variance_fit=latent_variance_fit)\n",
    "\n",
    "                losses_train[r,s,i,j,:len(ls_train)] = ls_train\n",
    "                losses_test[r,s,i,j,:len(ls_test)] = ls_test\n",
    "                n = 0\n",
    "                latents_train[r,s,i,j] = latent_mean_fit.detach().numpy()[n,:,0]\n",
    "                data_train_show[s] = data[n]\n",
    "                true_latents_train[s] = true_latent_ext[n,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bba5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "temporal = True\n",
    "\n",
    "model_seeds = [0,1,2]\n",
    "rpm_variant = 'amortized'\n",
    "amortize_ivi = 'use_q'\n",
    "inits = ['fits_noPretrain', 'fits_pretrainElbo', 'fits_pretrainAsQ']\n",
    "\n",
    "n33_losses_train = np.zeros((len(inits), len(model_seeds), 5000))\n",
    "n33_losses_test = np.zeros((len(inits), len(model_seeds), 5000))\n",
    "n33_latents_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), N, 50))\n",
    "n33_true_latents_train = np.zeros((len(model_seeds), N, 50))\n",
    "\n",
    "for s,model_seed in enumerate(model_seeds):\n",
    "    for r,res_dir in enumerate(inits):\n",
    "\n",
    "        if temporal:\n",
    "            identifier_folder = rpm_variant + '_temp_33Hidden_' + amortize_ivi + '_N'\n",
    "        else:\n",
    "            identifier_folder = rpm_variant + '_33Hidden_' + amortize_ivi + '_N'\n",
    "        identifier_folder = identifier_folder + '_' + str(N) + '_seed_' + str(model_seed)\n",
    "        if temporal:\n",
    "            identifier = rpm_variant + '_temp_' + amortize_ivi\n",
    "        else:\n",
    "            identifier = rpm_variant + '_' + amortize_ivi        \n",
    "        identifier = identifier + '_N_' + str(N) + '_seed_' + str(model_seed)\n",
    "\n",
    "        root = os.curdir\n",
    "        fn_base = os.path.join(res_dir, identifier_folder, identifier)\n",
    "\n",
    "        data = torch.tensor(np.load(fn_base + '_train_data.npy'))\n",
    "        true_latent_ext = torch.tensor(np.load(fn_base + '_train_latents.npy'))\n",
    "        #data = torch.tensor(np.load(fn_base + '_test_data.npy'))\n",
    "        #true_latent_ext = torch.tensor(np.load(fn_base + '_test_latents.npy'))\n",
    "\n",
    "        exp_dict = np.load(fn_base + '_exp_dict.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "        N,J,K,T = exp_dict['N'],exp_dict['J'],exp_dict['K'],exp_dict['T']\n",
    "        #init_diag_val = exp_dict['init_diag_val']\n",
    "        #init_off_val = exp_dict['init_off_val']\n",
    "        init_rb_bandwidth = exp_dict['init_rb_bandwidth']\n",
    "\n",
    "        ls_train = np.load(fn_base + '_loss_train.npy')\n",
    "        ls_test = np.load(fn_base + '_loss_test.npy')\n",
    "\n",
    "        xjs = [data[:,j] for j in range(J)]\n",
    "        pxjs = RPMEmpiricalMarginals(xjs)\n",
    "        observations = (torch.stack(xjs, dim=-1),)\n",
    "\n",
    "\n",
    "        model = init_gaussian_rpm(N, J, K, T, pxjs,\n",
    "                                init_rb_bandwidth, obs_locs,\n",
    "                                rpm_variant, temporal, amortize_ivi,\n",
    "                                epochs=0, batch_size=N,\n",
    "                                n_hidden=33\n",
    "                               )[0]\n",
    "        model.load_state_dict(torch.load(fn_base + '_rpm_state_dict'))\n",
    "\n",
    "\n",
    "        prior = model.joint_model[1]\n",
    "        eta_0 = prior.nat_param\n",
    "        if rpm_variant == 'amortized':    \n",
    "            eta_q, _ = model.comp_eta_q(xjs, eta_0)\n",
    "        else: \n",
    "            eta_q = model.comp_eta_q(xjs, eta_0=eta_0, idx_data=np.arange(N))\n",
    "        EqtZ = prior.log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "        mu = EqtZ[:,:T]\n",
    "        sig2 = torch.diagonal(EqtZ[:,T:].reshape(-1,T,T),dim1=-2,dim2=-1) - mu**2\n",
    "\n",
    "        latent_true, latent_mean_fit, latent_variance_fit, R2 = regLatent(\n",
    "            latent_true = true_latent_ext,\n",
    "            latent_mean_fit = mu.unsqueeze(-1), \n",
    "            latent_variance_fit = sig2)\n",
    "\n",
    "        #plot_poisson_balls(observations, \n",
    "        #                   obs_locs=obs_locs.squeeze(-1), \n",
    "        #                   latent_mean_fit=latent_mean_fit.squeeze(-1), \n",
    "        #                   latent_variance_fit=latent_variance_fit)\n",
    "\n",
    "        n=0\n",
    "        n33_losses_train[r,s] = ls_train\n",
    "        n33_losses_test[r,s] = ls_test\n",
    "        n33_latents_train[r,s,i,j] = latent_mean_fit.detach().numpy()[n,:,0]\n",
    "        n33_true_latents_train[s] = true_latent_ext[n,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b35d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_train\n",
    "filter_len = 1\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses[...,:100])\n",
    "l_max = np.nanmax(losses[...,:100])\n",
    "\n",
    "l_min = np.minimum(np.nanmin(n33_losses_train[...,:100]), l_min)\n",
    "l_max = np.maximum(np.nanmax(n33_losses_train[...,:100]), l_max)\n",
    "\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])                    \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        plt.ylim(l_min, l_max)\n",
    "        plt.xlim(0, 100)\n",
    "        loss_smoothed = np.convolve(n33_losses_train[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')\n",
    "        \n",
    "plt.legend()\n",
    "plt.suptitle('initial training losses, time-series model variant, Poisson balls, N='+str(N))\n",
    "#plt.savefig('figs/poisson_balls_temporal_initial_training_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_train\n",
    "filter_len = 50\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses)\n",
    "l_max = np.nanmax(losses)\n",
    "#l_min = np.minimum(np.nanmin(n33_losses_train), l_min)\n",
    "#l_max = np.maximum(np.nanmax(n33_losses_train), l_max)\n",
    "\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        plt.ylim(l_min, l_max)\n",
    "        #loss_smoothed = np.convolve(n33_losses_train[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        #plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')\n",
    "        \n",
    "plt.legend()\n",
    "plt.suptitle('training losses (time-smoothed), time-series model variant, Poisson balls, N='+str(N))\n",
    "#plt.savefig('figs/poisson_balls_temporal_full_training_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed194a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_test\n",
    "filter_len = 1\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses)\n",
    "l_max = np.nanmax(losses)\n",
    "l_min = np.minimum(np.nanmin(n33_losses_test), l_min)\n",
    "l_max = np.maximum(np.nanmax(n33_losses_test), l_max)\n",
    "\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        plt.ylim(l_min, l_max)\n",
    "        loss_smoothed = np.convolve(n33_losses_test[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')\n",
    "        \n",
    "plt.legend()\n",
    "plt.suptitle('test losses, Poisson balls, time-series model variant, N='+str(N))\n",
    "plt.savefig('figs/poisson_balls_temporal_full_test_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec17fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "n = 0\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(2*len(model_seeds), len(inits), 2*s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                plt.imshow(data_train_show[s,n], aspect='auto', origin='lower', cmap='gray',extent=[0, 1, -1, 1])\n",
    "                plt.plot(obs_locs, latents_train[r,s,i,j,n], \n",
    "                         color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j],\n",
    "                         linewidth=2.5)\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        #plt.plot(obs_locs, n33_latents_train[r,s,i,j,n], \n",
    "        #         color=colors[i,j], linestyle=':', label=rpm_variants[i]+'_'+amortize_ivis[j],\n",
    "        #         linewidth=2.5)\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])\n",
    "        plt.ylim(-1, 1)\n",
    "        ax = plt.subplot(2*len(model_seeds), len(inits), (2*s+1)*len(inits)+r+1)\n",
    "        MSEs = np.zeros((len(rpm_variants)*len(amortize_ivis)+1))\n",
    "        norm_ = (true_latents_train[s]**2).mean() # this is using that we see exactly 2pi of the true latents!\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                MSEs[i*len(amortize_ivis)+j] = ((latents_train[r,s,i,j]-true_latents_train[s])**2).mean()\n",
    "        #MSEs[-1] = ((n33_latents_train[r,s,i,j]-n33_true_latents_train[s])**2).mean()\n",
    "        plt.bar(np.arange(len(rpm_variants)*len(amortize_ivis)+1)+1, \n",
    "                MSEs/norm_,\n",
    "                color=np.concatenate(list([colors[i] for i in range(colors.shape[0])]) + [colors[-1,-1].reshape(1,-1)], axis=0)\n",
    "               )\n",
    "        plt.bar(np.arange(len(rpm_variants)*len(amortize_ivis))+1, \n",
    "                MSEs[:-1]/norm_,\n",
    "                color=np.concatenate([colors[i] for i in range(colors.shape[0])], axis=0)\n",
    "               )        \n",
    "        #plt.bar(len(rpm_variants)*len(amortize_ivis)+1, \n",
    "        #        MSEs[-1]/norm_,\n",
    "        #        color='w', \n",
    "        #        edgecolor=colors[-1,-1],\n",
    "        #        linestyle=':'\n",
    "        #       )        \n",
    "        plt.xticks([])\n",
    "        plt.ylabel('nMSEs')\n",
    "\n",
    "plt.suptitle('Latent means, time-series model variant, Poisson balls, N='+str(N))\n",
    "#plt.savefig('figs/poisson_balls_temporal_training_latents_='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84150df",
   "metadata": {},
   "source": [
    "# textured ball experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56ab29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_data_external import linear_regression_1D_latent as regLatent\n",
    "from utils_data_external import plot_poisson_balls\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_setup import init_gaussian_rpm\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "\n",
    "N = 10\n",
    "temporal = True\n",
    "\n",
    "rpm_variants = ['VI', 'VAE', 'amortized']\n",
    "amortize_ivis = ['none', 'full', 'use_q']\n",
    "model_seeds = [0,1,2]\n",
    "\n",
    "#inits = ['fits_noPretrain', 'fits_pretrainElbo', 'fits_pretrainAsQ', 'fits_reparam'] #\n",
    "inits = ['fits'] #['fits_reparam']\n",
    "obs_locs = torch.linspace(0,1,T).reshape(-1,1)\n",
    "\n",
    "losses_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), 5000))\n",
    "losses_test = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), 5000))\n",
    "\n",
    "latents_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), N, 50))\n",
    "true_latents_train = np.zeros((len(model_seeds), N, 50))\n",
    "data_train_show  = np.zeros((len(model_seeds), N, J, T))\n",
    "\n",
    "for s,model_seed in enumerate(model_seeds):\n",
    "    for r,res_dir in enumerate(inits):\n",
    "        for i,rpm_variant in enumerate(rpm_variants):\n",
    "            for j,amortize_ivi in enumerate(amortize_ivis):\n",
    "\n",
    "                if temporal:\n",
    "                    identifier = rpm_variant + '_temp_' + amortize_ivi\n",
    "                else:\n",
    "                    identifier = rpm_variant + '_' + amortize_ivi        \n",
    "                identifier = identifier + '_textured_N_' + str(N) + '_seed_' + str(model_seed)\n",
    "\n",
    "                root = os.curdir\n",
    "                fn_base = os.path.join(res_dir, identifier, identifier)\n",
    "\n",
    "                data = torch.tensor(np.load(fn_base + '_train_data.npy'))\n",
    "                true_latent_ext = torch.tensor(np.load(fn_base + '_train_latents.npy'))\n",
    "                #data = torch.tensor(np.load(fn_base + '_test_data.npy'))\n",
    "                #true_latent_ext = torch.tensor(np.load(fn_base + '_test_latents.npy'))\n",
    "\n",
    "                exp_dict = np.load(fn_base + '_exp_dict.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "                N,J,K,T = exp_dict['N'],exp_dict['J'],exp_dict['K'],exp_dict['T']\n",
    "                #init_diag_val = exp_dict['init_diag_val']\n",
    "                #init_off_val = exp_dict['init_off_val']\n",
    "                init_rb_bandwidth = exp_dict['init_rb_bandwidth']\n",
    "\n",
    "                ls_train = np.load(fn_base + '_loss_train.npy')\n",
    "                ls_test = np.load(fn_base + '_loss_test.npy')\n",
    "\n",
    "                xjs = [data[:,j] for j in range(J)]\n",
    "                pxjs = RPMEmpiricalMarginals(xjs)\n",
    "                observations = (torch.stack(xjs, dim=-1),)\n",
    "\n",
    "\n",
    "                model = init_gaussian_rpm(N, J, K, T, pxjs,\n",
    "                                        init_rb_bandwidth, obs_locs,\n",
    "                                        rpm_variant, temporal, amortize_ivi,\n",
    "                                        epochs=0, batch_size=N\n",
    "                                       )[0]\n",
    "                model.load_state_dict(torch.load(fn_base + '_rpm_state_dict'))\n",
    "\n",
    "\n",
    "                prior = model.joint_model[1]\n",
    "                eta_0 = prior.nat_param\n",
    "                if rpm_variant == 'amortized':    \n",
    "                    eta_q, _ = model.comp_eta_q(xjs, eta_0)\n",
    "                else: \n",
    "                    eta_q = model.comp_eta_q(xjs, eta_0=eta_0, idx_data=np.arange(N))\n",
    "                EqtZ = prior.log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "                mu = EqtZ[:,:T]\n",
    "                sig2 = torch.diagonal(EqtZ[:,T:].reshape(-1,T,T),dim1=-2,dim2=-1) - mu**2\n",
    "\n",
    "                latent_true, latent_mean_fit, latent_variance_fit, R2 = regLatent(\n",
    "                    latent_true = true_latent_ext,\n",
    "                    latent_mean_fit = mu.unsqueeze(-1), \n",
    "                    latent_variance_fit = sig2)\n",
    "\n",
    "                #plot_poisson_balls(observations, \n",
    "                #                   obs_locs=obs_locs.squeeze(-1), \n",
    "                #                   latent_mean_fit=latent_mean_fit.squeeze(-1), \n",
    "                #                   latent_variance_fit=latent_variance_fit)\n",
    "\n",
    "                losses_train[r,s,i,j,:len(ls_train)] = ls_train\n",
    "                losses_test[r,s,i,j,:len(ls_test)] = ls_test\n",
    "                n = 0\n",
    "                latents_train[r,s,i,j] = latent_mean_fit.detach().numpy()[n,:,0]\n",
    "                data_train_show[s] = data[n]\n",
    "                true_latents_train[s] = true_latent_ext[n,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(poisson_ball, cmap='gray')\n",
    "plt.xticks([0, 24, 49], [1, 25, 50])\n",
    "plt.yticks([0,9], [10,1])\n",
    "plt.xlabel('time t')\n",
    "plt.ylabel('space j')\n",
    "plt.title('Poisson bouncing ball')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(data_train_show[1,1], cmap='gray')\n",
    "plt.xticks([0, 24, 49], [1, 25, 50])\n",
    "plt.yticks([0,9], [10,1])\n",
    "plt.xlabel('time t')\n",
    "plt.title('Textured bouncing ball')\n",
    "\n",
    "#plt.savefig('figs/bouncing_balls_examples.pdf', bbox_inches ='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 10\n",
    "temporal = True\n",
    "\n",
    "model_seeds = [0,1,2]\n",
    "rpm_variant = 'amortized'\n",
    "amortize_ivi = 'use_q'\n",
    "inits = ['fits_noPretrain', 'fits_pretrainElbo', 'fits_pretrainAsQ']\n",
    "\n",
    "n33_losses_train = np.zeros((len(inits), len(model_seeds), 5000))\n",
    "n33_losses_test = np.zeros((len(inits), len(model_seeds), 5000))\n",
    "n33_latents_train = np.zeros((len(inits), len(model_seeds), len(rpm_variants), len(amortize_ivis), N, 50))\n",
    "n33_true_latents_train = np.zeros((len(model_seeds), N, 50))\n",
    "\n",
    "for s,model_seed in enumerate(model_seeds):\n",
    "    for r,res_dir in enumerate(inits):\n",
    "\n",
    "        if temporal:\n",
    "            identifier_folder = rpm_variant + '_temp_33Hidden_' + amortize_ivi + '_N'\n",
    "        else:\n",
    "            identifier_folder = rpm_variant + '_33Hidden_' + amortize_ivi + '_N'\n",
    "        identifier_folder = identifier_folder + '_' + str(N) + '_seed_' + str(model_seed)\n",
    "        if temporal:\n",
    "            identifier = rpm_variant + '_temp_' + amortize_ivi\n",
    "        else:\n",
    "            identifier = rpm_variant + '_' + amortize_ivi        \n",
    "        identifier = identifier + '_N_' + str(N) + '_seed_' + str(model_seed)\n",
    "\n",
    "        root = os.curdir\n",
    "        fn_base = os.path.join(res_dir, identifier_folder, identifier)\n",
    "\n",
    "        data = torch.tensor(np.load(fn_base + '_train_data.npy'))\n",
    "        true_latent_ext = torch.tensor(np.load(fn_base + '_train_latents.npy'))\n",
    "        #data = torch.tensor(np.load(fn_base + '_test_data.npy'))\n",
    "        #true_latent_ext = torch.tensor(np.load(fn_base + '_test_latents.npy'))\n",
    "\n",
    "        exp_dict = np.load(fn_base + '_exp_dict.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "        N,J,K,T = exp_dict['N'],exp_dict['J'],exp_dict['K'],exp_dict['T']\n",
    "        #init_diag_val = exp_dict['init_diag_val']\n",
    "        #init_off_val = exp_dict['init_off_val']\n",
    "        init_rb_bandwidth = exp_dict['init_rb_bandwidth']\n",
    "\n",
    "        ls_train = np.load(fn_base + '_loss_train.npy')\n",
    "        ls_test = np.load(fn_base + '_loss_test.npy')\n",
    "\n",
    "        xjs = [data[:,j] for j in range(J)]\n",
    "        pxjs = RPMEmpiricalMarginals(xjs)\n",
    "        observations = (torch.stack(xjs, dim=-1),)\n",
    "\n",
    "\n",
    "        model = init_gaussian_rpm(N, J, K, T, pxjs,\n",
    "                                init_rb_bandwidth, obs_locs,\n",
    "                                rpm_variant, temporal, amortize_ivi,\n",
    "                                epochs=0, batch_size=N,\n",
    "                                n_hidden=33\n",
    "                               )[0]\n",
    "        model.load_state_dict(torch.load(fn_base + '_rpm_state_dict'))\n",
    "\n",
    "\n",
    "        prior = model.joint_model[1]\n",
    "        eta_0 = prior.nat_param\n",
    "        if rpm_variant == 'amortized':    \n",
    "            eta_q, _ = model.comp_eta_q(xjs, eta_0)\n",
    "        else: \n",
    "            eta_q = model.comp_eta_q(xjs, eta_0=eta_0, idx_data=np.arange(N))\n",
    "        EqtZ = prior.log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "        mu = EqtZ[:,:T]\n",
    "        sig2 = torch.diagonal(EqtZ[:,T:].reshape(-1,T,T),dim1=-2,dim2=-1) - mu**2\n",
    "\n",
    "        latent_true, latent_mean_fit, latent_variance_fit, R2 = regLatent(\n",
    "            latent_true = true_latent_ext,\n",
    "            latent_mean_fit = mu.unsqueeze(-1), \n",
    "            latent_variance_fit = sig2)\n",
    "\n",
    "        #plot_poisson_balls(observations, \n",
    "        #                   obs_locs=obs_locs.squeeze(-1), \n",
    "        #                   latent_mean_fit=latent_mean_fit.squeeze(-1), \n",
    "        #                   latent_variance_fit=latent_variance_fit)\n",
    "\n",
    "        n=0\n",
    "        n33_losses_train[r,s] = ls_train\n",
    "        n33_losses_test[r,s] = ls_test\n",
    "        n33_latents_train[r,s,i,j] = latent_mean_fit.detach().numpy()[n,:,0]\n",
    "        n33_true_latents_train[s] = true_latent_ext[n,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313604b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_train\n",
    "filter_len = 1\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses[...,:100])\n",
    "l_max = np.nanmax(losses[...,:100])\n",
    "l_min = np.minimum(np.nanmin(n33_losses_train[...,:100]), l_min)\n",
    "l_max = np.maximum(np.nanmax(n33_losses_train[...,:100]), l_max)\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])                    \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        plt.ylim(l_min, l_max)\n",
    "        plt.xlim(0, 100)\n",
    "        loss_smoothed = np.convolve(n33_losses_train[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')        \n",
    "plt.legend()\n",
    "plt.suptitle('initial training losses, time-series model variant, textured balls, N='+str(N))\n",
    "#plt.savefig('figs/textured_balls_temporal_initial_training_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_train\n",
    "filter_len = 50\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses)\n",
    "l_max = np.nanmax(losses)\n",
    "#l_min = np.minimum(np.nanmin(n33_losses_train), l_min)\n",
    "#l_max = np.maximum(np.nanmax(n33_losses_train), l_max)\n",
    "\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        try:\n",
    "            plt.ylim(l_min, l_max)\n",
    "        except:\n",
    "            pass\n",
    "        #loss_smoothed = np.convolve(n33_losses_train[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')        \n",
    "plt.legend()\n",
    "plt.suptitle('training losses (time-smoothed), time-series model variant, textured balls, N='+str(N))\n",
    "#plt.savefig('figs/textured_balls_temporal_full_training_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c97da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_test\n",
    "filter_len = 1\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "l_min = np.nanmin(losses)\n",
    "l_max = np.nanmax(losses)\n",
    "l_min = np.minimum(np.nanmin(n33_losses_test), l_min)\n",
    "l_max = np.maximum(np.nanmax(n33_losses_test), l_max)\n",
    "\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(len(model_seeds), len(inits), s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                loss_smoothed = np.convolve(losses[r,s,i,j], np.ones(filter_len)/filter_len, 'valid')\n",
    "                plt.plot(loss_smoothed, color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j])\n",
    "                #plt.plot(losses[s, i, j], color=colors[i,j], label=amortize_ivis[j]+'_'+rpm_variants[i])\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])                    \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        if s == len(model_seeds)-1:\n",
    "            plt.xlabel('epochs')\n",
    "        plt.ylim(l_min, l_max)\n",
    "        loss_smoothed = np.convolve(n33_losses_test[r,s], np.ones(filter_len)/filter_len, 'valid')\n",
    "        plt.plot(loss_smoothed, ':', color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j] + '_33hidden')        \n",
    "plt.legend()\n",
    "plt.suptitle('test losses, textured balls, time-series model variant, N='+str(N))\n",
    "plt.savefig('figs/textured_balls_temporal_full_test_losses_N='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "n = 0\n",
    "for r in range(len(inits)):\n",
    "    for s in range(len(model_seeds)):\n",
    "        ax = plt.subplot(2*len(model_seeds), len(inits), 2*s*len(inits)+r+1)\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                plt.imshow(data_train_show[s,n], aspect='auto', origin='lower', cmap='gray',extent=[0, 1, -1, 1])\n",
    "                plt.plot(obs_locs, latents_train[r,s,i,j,n], \n",
    "                         color=colors[i,j], label=rpm_variants[i]+'_'+amortize_ivis[j],\n",
    "                         linewidth=2.5)\n",
    "                if r == 0:\n",
    "                    plt.ylabel('seed #' + str(s+1))\n",
    "        plt.plot(obs_locs, n33_latents_train[r,s,i,j,n], \n",
    "                 color=colors[i,j], linestyle=':', label=rpm_variants[i]+'_'+amortize_ivis[j],\n",
    "                 linewidth=2.5)\n",
    "        if s == 0:\n",
    "            plt.title(inits[r])\n",
    "        plt.ylim(-1, 1)\n",
    "        ax = plt.subplot(2*len(model_seeds), len(inits), (2*s+1)*len(inits)+r+1)\n",
    "        MSEs = np.zeros((len(rpm_variants)*len(amortize_ivis)+1))\n",
    "        norm_ = (true_latents_train[s]**2).mean() # this is using that we see exactly 2pi of the true latents!\n",
    "        for i in range(len(rpm_variants)):\n",
    "            for j in range(len(amortize_ivis)):\n",
    "                MSEs[i*len(amortize_ivis)+j] = ((latents_train[r,s,i,j]-true_latents_train[s])**2).mean()\n",
    "        MSEs[-1] = ((n33_latents_train[r,s,i,j]-n33_true_latents_train[s])**2).mean()\n",
    "        #plt.bar(np.arange(len(rpm_variants)*len(amortize_ivis)+1)+1, \n",
    "        #        MSEs/norm_,\n",
    "        #        color=np.concatenate(list([colors[i] for i in range(colors.shape[0])]) + [colors[-1,-1].reshape(1,-1)], axis=0)\n",
    "        #       )\n",
    "        plt.bar(np.arange(len(rpm_variants)*len(amortize_ivis))+1, \n",
    "                MSEs[:-1]/norm_,\n",
    "                color=np.concatenate([colors[i] for i in range(colors.shape[0])], axis=0)\n",
    "               )        \n",
    "        plt.bar(len(rpm_variants)*len(amortize_ivis)+1, \n",
    "                MSEs[-1]/norm_,\n",
    "                color='w', \n",
    "                edgecolor=colors[-1,-1],\n",
    "                linestyle=':'\n",
    "               )        \n",
    "        plt.xticks([])\n",
    "        plt.ylabel('nMSEs')\n",
    "        \n",
    "plt.suptitle('Latent means, time-series model variant, textured balls, N='+str(N))\n",
    "plt.savefig('figs/textured_balls_temporal_training_latents_='+str(N) + '_added33Hidden.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85570d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 2,2\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "prior = model.joint_model[1]\n",
    "full2diag_gaussian = prior.log_partition.full2diag_gaussian\n",
    "eta_0 = prior.nat_param\n",
    "eta_0_diag, eta_0_uncorr = full2diag_gaussian(eta_0)\n",
    "etajs_all = prior.log_partition.extract_diagonal(model.factorNatParams(eta_off=eta_0_uncorr)) # N-by-J-by-D\n",
    "etajs_all = etajs_all.reshape(*etajs_all.shape[:-1],2,T).transpose(-1,-2)                    # N-by-J-by-T-by-2\n",
    "marginal_log_partition = prior.log_partition.marginal_log_partition \n",
    "phijs_all = marginal_log_partition(etajs_all)                                                # N-by-J-by-T\n",
    "\n",
    "phi0 = marginal_log_partition(eta_0_diag)\n",
    "Z = torch.linspace(-5, 5, 200)\n",
    "tZ = torch.stack([Z, Z**2], axis=1)\n",
    "pZ = torch.exp((eta_0_diag[0].unsqueeze(1) * tZ.unsqueeze(0)).sum(axis=-1) - phi0.T)/ np.sqrt(2*np.pi)\n",
    "\n",
    "for jj in range(J):\n",
    "    etaj_all = etajs_all[:,jj]\n",
    "    phij_all = phijs_all[:,jj]\n",
    "    fj = torch.exp((etaj_all.unsqueeze(0) * tZ.unsqueeze(1).unsqueeze(1)).sum(axis=-1) - phij_all.unsqueeze(0))\n",
    "    Fj = fj.mean(axis=1) / np.sqrt(2*np.pi)\n",
    "    plt.plot(Z.detach().numpy(), Fj.detach().numpy(), label='Fj(Z), j='+str(j+1), color=colors[i,j])\n",
    "plt.plot(Z.detach().numpy(), pZ.detach().numpy().T, label='prior p(Z)', color='cyan')\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('density')\n",
    "plt.title('Z-marginals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73747270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
