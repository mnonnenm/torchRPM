{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "from discreteRPM import discreteRPM, Prior_discrete, RecognitionFactor_discrete\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "J = 2                           # three marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "K = 7\n",
    "dim_T = K                       # dimension of sufficient statistics\n",
    "\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'exponential' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    #A = np.random.normal(size=(J,J))\n",
    "    #P = A.dot(A.T)\n",
    "    #P = P / np.sqrt(np.outer(np.diag(P), np.diag(P)))    \n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "\n",
    "\n",
    "# define Gaussian prior in natural parametrization  \n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization  \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "natparam_models = [Net(dim_js[j], K, n_hidden=13) for j in range(J)]\n",
    "rec_models = [RecognitionFactor_discrete(model=m) for m in natparam_models]\n",
    "\n",
    "# constsruct implicit RPM\n",
    "drpm = discreteRPM( rec_models, latent_prior=Prior_discrete(param=torch.randn(size=(K,))), pxjs=pxjs )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119fcdd",
   "metadata": {},
   "source": [
    "# check analytical gradients numerically\n",
    "\n",
    "With loss $\\mathcal{L}_N(\\theta) = \\sum_n \\omega_\\theta(x^{(n)})$, we can get gradients $\\nabla\\mathcal{L}_N(\\theta)$ either via auto-differentiation using $\\omega_\\theta(x) = \\int \\prod_j \\frac{f_{\\theta_j}(Z|x_j)}{F_{\\theta_j}(Z)} p_\\theta(Z) dZ$ or alternatively via\n",
    "\n",
    "$\\nabla{}\\mathcal{L}_N(\\theta) = \\sum_n \\frac{\\partial\\eta_j^{(n)}}{\\partial\\theta_j}^\\top \\left( \\mathbb{E}[t(Z)|x^{(n)}, \\theta] - \\frac{\\tilde{p}_{N,\\theta}(x_j^{(n)})}{p_j(x_j^{(n)})} \\tilde{\\mathbb{E}}[t(Z)|x_j^{(n)}, \\theta] + (\\frac{\\tilde{p}_{N,\\theta}(x_j^{(n)})}{p_j(x_j^{(n)})}-1) \\frac{\\partial\\Phi(\\eta_j(x_j^{(n)}))}{\\partial\\theta_j}\\right) $\n",
    "\n",
    "where $\\tilde{p}$ signifies the RPM with substituted prior $p_\\theta(Z) \\leftarrow \\tilde{p}_{N,\\theta}(Z) = \\frac{1}{N}\\sum_n p_{N,\\theta}(Z|x^{(n)})$ and otherwise same model components $f_{\\theta_j}(Z|xj), \\ p_j(x_j)$.\n",
    "\n",
    "Here we check the validity of of the above form written in terms of posterior expectations over $t(Z)$ against pytorch auto-diff gradients using $\\omega_\\theta(x)$, using that in the discrete we can easily compute all involved integrals over $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a74ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functorch import make_functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 23\n",
    "xjs = [torch.randn(size=(N,dim_js[j])) for j in range(drpm.J)]\n",
    "logw, posterior, w_tilda_j, posterior_tilda_j  = drpm.eval(xjs)\n",
    "\n",
    "drpm.zero_grad()\n",
    "loss = drpm.eval(xjs)[0].mean() # average negative log(w(x))\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    j = 0\n",
    "    m = drpm.rec_models[j]\n",
    "    frec_model, params = make_functional(m)\n",
    "    detaj_dthetajs = torch.func.jacrev(frec_model, argnums=0)(params, xjs[j])\n",
    "    \n",
    "    tmp = (posterior-w_tilda_j[:,j].reshape(-1,1)*posterior_tilda_j[:,j] + (w_tilda_j[:,j].reshape(-1,1)-1.0)*torch.exp(m.log_probs(xjs[j])))\n",
    "    grad_ana = []\n",
    "    for k in range(len(params)):\n",
    "        detaj_dthetajk = detaj_dthetajs[k]\n",
    "        tmpk = detaj_dthetajk*(tmp.reshape(*tmp.shape, 1, 1)) if detaj_dthetajk.ndim==4 else detaj_dthetajk*(tmp.reshape(*tmp.shape, 1))\n",
    "        grad_ana.append(\n",
    "            1. * tmpk.sum(axis=1).mean(axis=0)\n",
    "        )\n",
    "        print(\"\\n analytical \\n\")\n",
    "        print(grad_ana[-1])\n",
    "        plt.plot(grad_ana[-1].detach())\n",
    "        for i,p in enumerate(drpm.parameters()):\n",
    "            if i == k:\n",
    "                plt.plot(p.grad.detach(), '--')\n",
    "                print(\"\\n numerical \\n\")\n",
    "                print(p.grad)\n",
    "                break\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(drpm.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 500\n",
    "N = 100\n",
    "batch_size = 10\n",
    "\n",
    "class RPMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,xjs):\n",
    "        self.J = len(xjs)\n",
    "        assert all([len(xjs[0]) == len(xjs[j]) for j in range(self.J)])\n",
    "        self.xjs = xjs\n",
    "    def __len__(self):\n",
    "        return len(self.xjs[0])\n",
    "    def __getitem__(self,idx):\n",
    "        return [self.xjs[j][idx] for j in range(self.J)]\n",
    "\n",
    "ds = RPMDataset(px.sample_n(n=N))\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = drpm.training_step(batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 3:\n",
    "    XX,YY,ZZ = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten(), ZZ.flatten()], axis=-1)\n",
    "    logpx = drpm.eval([XX.reshape(-1,1), YY.reshape(-1,1), ZZ.reshape(-1,1)])[0].reshape(100,100,100)\n",
    "elif J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    logpx = drpm.eval([XX.reshape(-1,1), YY.reshape(-1,1)])[0].reshape(100,100)\n",
    "\n",
    "if J == 2:\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(logpx.detach().numpy(), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under dRPM with copula loss')\n",
    "    #plt.plot(xjs05[1], xjs05[0], 'ro')\n",
    "    #xjs= pxind.sample_n(50)\n",
    "    #plt.plot(xjs[1], xjs[0], 'kx')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.contour(YY, XX , logpx.detach().numpy(), levels=5)\n",
    "    plt.ylabel(r'$x_1$')\n",
    "    plt.xlabel(r'$x_2$')\n",
    "    plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "    plt.title('log p(x) under dRPM with copula loss')\n",
    "    #plt.plot(xjs05[1], xjs05[0], 'ro')\n",
    "    #xjs= pxind.sample_n(50)\n",
    "    #plt.plot(xjs[1], xjs[0], 'kx')\n",
    "    xjs = px.sample_n(n=1000)\n",
    "    plt.plot(xjs[1], xjs[0], 'r.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ecea28",
   "metadata": {},
   "source": [
    "# RPM without conditional independence assumption\n",
    "- dependency structure is overrated !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c8604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution\n",
    "from discreteRPM import discreteRPM, discretenonCondIndRPM, Prior_discrete, RecognitionFactor_discrete, RecognitionFactor_scaled_discrete\n",
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "J = 2                           # three marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "K = 20\n",
    "dim_T = K                       # dimension of sufficient statistics\n",
    "\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'exponential' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    #A = np.random.normal(size=(J,J))\n",
    "    #P = A.dot(A.T)\n",
    "    #P = P / np.sqrt(np.outer(np.diag(P), np.diag(P)))    \n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "\n",
    "\n",
    "# define Gaussian prior in natural parametrization  \n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization  \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "#natparam_model = Net(sum(dim_js), K, n_hidden=13)\n",
    "#rec_model = RecognitionFactor_discrete(model=natparam_model) \n",
    "natparam_model = Net(sum(dim_js), K+1, n_hidden=50)\n",
    "rec_model = RecognitionFactor_scaled_discrete(model=natparam_model) \n",
    "\n",
    "N = 5000\n",
    "xjs = px.sample_n(n=N)\n",
    "pxj = RPMEmpiricalMarginals(xjs)\n",
    "\n",
    "# constsruct implicit RPM\n",
    "drpm = discretenonCondIndRPM( rec_model, latent_prior=Prior_discrete(param=torch.randn(size=(K,))), pxjs=pxj, full_F=False )\n",
    "\n",
    "\n",
    "natparam_models = [Net(dim_js[j], K, n_hidden=50) for j in range(J)]\n",
    "rec_models = [RecognitionFactor_discrete(model=m) for m in natparam_models]\n",
    "drpm = discreteRPM( rec_models, latent_prior=Prior_discrete(param=torch.randn(size=(K,))), pxjs=pxjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(drpm.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "xs = torch.stack(xjs, axis=1)\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(xs)\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = drpm.training_step(batch[0] if len(batch)==1 else batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3de1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "\n",
    "drpm.full_F = True\n",
    "log_w = drpm.eval([xgrid[:,j] for j in range(J)])[0].reshape(100,100)\n",
    "logpx = log_w + torch.log(torch.Tensor(rates)[0]) - rates[0] * xxs[0].reshape(-1,1) + torch.log(torch.Tensor(rates)[1]) - rates[1] * xxs[1].reshape(1,-1)\n",
    "drpm.full_F = False\n",
    "\n",
    "logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.exp(logpx_true), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('true log p(x) and samples')\n",
    "plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('log p(x) under dRPM')\n",
    "#plt.plot(xjs[1], xjs[0], 'r.')\n",
    "\n",
    "\"\"\"\n",
    "plt.subplot(1,3,3)\n",
    "m = drpm.rec_model\n",
    "log_fxs = m.log_probs(xgrid)                                     # N^J       - K \n",
    "log_denom = torch.logsumexp(log_fxs,dim=0).reshape(1,-1) - np.log(len(xgrid)**J) # 1         - K\n",
    "pOverF = (torch.exp(drpm.latent_prior.log_probs()).reshape(1,-1)/torch.exp(log_denom).reshape(1,-1)).detach().numpy()\n",
    "posts = torch.exp(log_fxs).detach().numpy() * pOverF \n",
    "posts = posts.reshape(100,100,7)\n",
    "z_posts = np.argmax(posts, axis=-1)\n",
    "plt.imshow(z_posts)\n",
    "plt.colorbar()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('log p(x) under dRPM')\n",
    "plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(torch.exp(drpm.rec_model.model(xgrid)[:,-1]).detach().numpy().reshape(100,100), \n",
    "           origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('log p(x) under dRPM')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "\n",
    "    \n",
    "drpm.full_F = True\n",
    "log_w = drpm.eval([xgrid[:,j] for j in range(J)])[0].reshape(100,100)\n",
    "drpm.full_F = False\n",
    "\n",
    "log_p0 = torch.log(torch.Tensor(rates)[0]) - rates[0] * xxs[0].reshape(-1,1) + torch.log(torch.Tensor(rates)[1]) - rates[1] * xxs[1].reshape(1,-1)\n",
    "logpx = log_w + log_p0\n",
    "\n",
    "\n",
    "logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.exp(logpx_true), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('true p(x) and samples')\n",
    "plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('p(x) under dRPM')\n",
    "#plt.plot(xjs[1], xjs[0], 'r.')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "log_fs = torch.stack([m(xgrid[:,j]) for j,m in enumerate(drpm.rec_models)], dim=1)\n",
    "log_norm_f =  np.log(torch.exp(log_fs.sum(axis=1)).sum(axis=-1).detach().numpy().reshape(100,100))\n",
    "log_norm = log_norm_f + log_p0.detach().numpy()\n",
    "\n",
    "plt.imshow(np.exp(log_norm),\n",
    "           origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title(r'normalization of $\\prod_j \\ f(Z|x_j)$')\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75327f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f314b8e",
   "metadata": {},
   "source": [
    "# RP-VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d790ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class discreteRPVAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Variational auto-encoder for recognition-Parametrized Model (RPM)\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_models, latent_prior, pxjs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.J = len(rec_models)\n",
    "        assert len(pxjs) == self.J\n",
    "\n",
    "        self.rec_models = torch.nn.ModuleList(rec_models)\n",
    "        self.latent_prior = latent_prior\n",
    "        self.pxjs = pxjs\n",
    "\n",
    "    def eval(self, xjs):\n",
    "        J = self.J\n",
    "        assert len(xjs) == J\n",
    "        N = xjs[0].shape[0]\n",
    "        assert all([xjs[j].shape[0] == N for j in range(self.J)])\n",
    "\n",
    "        log_fnji = torch.stack([m.log_probs(xj)for m,xj in zip(self.rec_models, xjs)], axis=1)  # N-J-K\n",
    "        Fji  = torch.exp(log_fnji).mean(axis=0)                                                 #   J-K\n",
    "        log_prod_Fj = torch.log(Fji).sum(axis=0).reshape(1,-1)                                  # 1 - K\n",
    "        log_prod_fj = log_fnji.sum(axis=1)                                                      # N - K\n",
    "        log_prod_frac = log_prod_fj - log_prod_Fj                                               # N - K\n",
    "\n",
    "        log_joint_factor = self.latent_prior.log_probs() + log_prod_frac                        # N - K\n",
    "        logw = torch.log(torch.exp(log_joint_factor).sum(axis=-1))                              # N\n",
    "\n",
    "        #posterior = torch.exp(log_joint_factor - logw.reshape(-1,1))                            # N - K\n",
    "\n",
    "        #log_prior_tilda = torch.log(posterior.mean(axis=0)).reshape(1,-1)                       # 1 - K\n",
    "        #log_joint_tilda = log_prior_tilda + log_prod_frac                                       # N - K\n",
    "        #w_tilda = torch.exp(log_joint_tilda).sum(axis=-1)                                       # N\n",
    "\n",
    "        #log_joint_tilda_j = log_fnji + (log_prior_tilda - torch.log(Fji)).reshape(1,*Fji.shape) # N-J-K        \n",
    "        #w_tilda_j = torch.exp(log_joint_tilda_j).sum(axis=-1)                                   # N-J        \n",
    "        #posterior_tilda_j = torch.exp(log_joint_tilda_j) / w_tilda_j.reshape(-1,J,1)            # N-J-K\n",
    "\n",
    "        return logw\n",
    "\n",
    "    def elbo(self, xjs):\n",
    "        J = self.J\n",
    "        assert len(xjs) == J\n",
    "        N = xjs[0].shape[0]\n",
    "        assert all([xjs[j].shape[0] == N for j in range(self.J)])\n",
    "\n",
    "        log_fnji = torch.stack([m.log_probs(xj)for m,xj in zip(self.rec_models, xjs)], axis=1)  # N-J-K\n",
    "        Fji  = torch.exp(log_fnji).mean(axis=0)                                                 #   J-K\n",
    "        log_prod_Fj = torch.log(Fji).sum(axis=0).reshape(1,-1)                                  # 1 - K\n",
    "        log_prod_fj = log_fnji.sum(axis=1)                                                      # N - K\n",
    "\n",
    "        log_q = (1-J) * self.latent_prior.log_probs() + log_prod_fj                             # N - K\n",
    "        lognorm_q = torch.logsumexp(log_q, axis=1).unsqueeze(-1)                                # N - 1 \n",
    "        log_q = log_q - lognorm_q                                                               # N - K\n",
    "        \n",
    "        log_ratio = lognorm_q + (J * self.latent_prior.log_probs() - log_prod_Fj)               # N - K\n",
    "        elbo = (torch.exp(log_q) * log_ratio).sum(axis=1)                                       # N \n",
    "\n",
    "        return elbo\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # score matching loss\n",
    "        xjs = batch\n",
    "        loss = - self.elbo(xjs).mean() \n",
    "        return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpm import RPMEmpiricalMarginals, EmpiricalDistribution\n",
    "from discreteRPM import discreteRPM, discretenonCondIndRPM, Prior_discrete, RecognitionFactor_discrete, RecognitionFactor_scaled_discrete\n",
    "from implicitRPM import ObservedMarginal, IndependentMarginal, GaussianCopula_ExponentialMarginals\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "J = 2                           # three marginals \n",
    "dim_js = [1 for j in range(J)]  # dimensions of marginals\n",
    "dim_Z = 1                       # dimension of latent\n",
    "K = 20\n",
    "dim_T = K                       # dimension of sufficient statistics\n",
    "\n",
    "\n",
    "# currently playing with either Gaussian or Exponential marginals\n",
    "marginals = 'exponential' \n",
    "if marginals == 'exponential':\n",
    "    rates = [1.0, 0.5, 3.0][:J]\n",
    "    pxjs = [ObservedMarginal(torch.distributions.exponential.Exponential(rate=rates[j])) for j in range(J)]\n",
    "    #A = np.random.normal(size=(J,J))\n",
    "    #P = A.dot(A.T)\n",
    "    #P = P / np.sqrt(np.outer(np.diag(P), np.diag(P)))    \n",
    "    P = np.array([[1.0, -0.85], [-0.85, 1.0]])\n",
    "    print('P:', P)\n",
    "    px = GaussianCopula_ExponentialMarginals(P=P, rates=rates, dims=dim_js)\n",
    "elif marginals == 'gaussian':\n",
    "    locs, scales = [-1.5, -0.5, 3.0][:J], [1.0, 2.0, 0.25][:J]\n",
    "    pxjs = [torch.distributions.normal.Normal(loc=locs[j], scale=scales[j]) for j in range(J)]\n",
    "else: \n",
    "    raise Exception('marginals not implemented')\n",
    "pxind = IndependentMarginal(pxjs, dims=dim_js)\n",
    "\n",
    "\n",
    "# define Gaussian prior in natural parametrization  \n",
    "def activation_out(x,d=1): # NN returns natural parameters; in Gaussian case, that is m/sig2, -1/(2*sig2)\n",
    "    return torch.cat([x[:,:d], -torch.nn.Softplus()(x[:,d:])],axis=-1)\n",
    "\n",
    "# define Gaussian factors fj(Z|xj) in natural parametrization  \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, activation_out=torch.nn.Identity()):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation_out = activation_out\n",
    "        self.fc1 = torch.nn.Linear(n_in, n_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.activation_out(x)\n",
    "\n",
    "natparam_models = [Net(dim_js[j], K, n_hidden=50) for j in range(J)]\n",
    "rec_models = [RecognitionFactor_discrete(model=m) for m in natparam_models]\n",
    "drpm = discreteRPVAE( rec_models, latent_prior=Prior_discrete(param=torch.randn(size=(K,))), pxjs=pxjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "xjs = px.sample_n(n=N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optimizer = torch.optim.Adam(drpm.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(*xjs)\n",
    "dl = torch.utils.data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ls,t = np.zeros(epochs*(N//batch_size)),0\n",
    "for i in range(epochs):\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        loss = drpm.training_step(batch[0] if len(batch)==1 else batch, batch_idx=t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ls[t] = loss.detach().numpy()\n",
    "        t+=1\n",
    "plt.plot(ls)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# super lazy marginals for the RPM: numerically integrate over p(x1, ..., xJ) via below grid for J=2 or J=3\n",
    "if marginals == 'gaussian':\n",
    "    xxs = [torch.linspace(locs[j]-3*scales[j], locs[j]+3*scales[j],100) for j in range(J)]\n",
    "elif marginals == 'exponential':\n",
    "    xxs = [torch.linspace(0.001, 3/rates[j],100) for j in range(J)]\n",
    "if J == 2:\n",
    "    XX,YY = torch.meshgrid(*xxs)\n",
    "    xgrid = torch.stack([XX.flatten(), YY.flatten()], axis=-1)\n",
    "    xgrid = xgrid.reshape(*xgrid.shape, 1)\n",
    "\n",
    "    \n",
    "drpm.full_F = True\n",
    "log_w = drpm.eval([xgrid[:,j] for j in range(J)]).reshape(100,100)\n",
    "drpm.full_F = False\n",
    "\n",
    "log_p0 = torch.log(torch.Tensor(rates)[0]) - rates[0] * xxs[0].reshape(-1,1) + torch.log(torch.Tensor(rates)[1]) - rates[1] * xxs[1].reshape(1,-1)\n",
    "logpx = log_w + log_p0\n",
    "\n",
    "\n",
    "logpx_true = px.log_probs(xgrid.detach().numpy().squeeze(-1)).reshape(100,100)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.exp(logpx_true), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('true p(x) and samples')\n",
    "plt.plot(xjs[1], xjs[0], 'r.', markersize=0.5)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.exp(logpx.detach().numpy()), origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title('p(x) under dRPM')\n",
    "#plt.plot(xjs[1], xjs[0], 'r.')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "log_fs = torch.stack([m(xgrid[:,j]) for j,m in enumerate(drpm.rec_models)], dim=1)\n",
    "log_norm_f =  np.log(torch.exp(log_fs.sum(axis=1)).sum(axis=-1).detach().numpy().reshape(100,100))\n",
    "log_norm = log_norm_f + log_p0.detach().numpy()\n",
    "\n",
    "plt.imshow(np.exp(log_norm),\n",
    "           origin='lower', extent=(0., 3/rates[1], 0., 3/rates[0]), aspect='auto')\n",
    "plt.ylabel(r'$x_1$')\n",
    "plt.xlabel(r'$x_2$')\n",
    "plt.axis((0., 3/rates[1], 0., 3/rates[0]))\n",
    "plt.title(r'normalization of $\\prod_j \\ f(Z|x_j)$')\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
