{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2dbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884c290b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "running exp VAE_temp_none_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.732718825340271\n",
      "epoch #50/500, loss=0.6612141132354736\n",
      "epoch #100/500, loss=0.3623542785644531\n",
      "epoch #150/500, loss=0.06098340079188347\n",
      "epoch #200/500, loss=0.02180153876543045\n",
      "epoch #250/500, loss=0.01788211241364479\n",
      "epoch #300/500, loss=0.012884342111647129\n",
      "epoch #350/500, loss=0.011097465641796589\n",
      "epoch #400/500, loss=0.009344897232949734\n",
      "epoch #450/500, loss=0.007478659972548485\n",
      "done fitting.\n",
      "choosing initial values for ivi parameters to match q\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=34.69816970825195, test loss=nan\n",
      "epoch #50/500, train loss=25.343509674072266, test loss=nan\n",
      "epoch #100/500, train loss=11.54823112487793, test loss=nan\n",
      "epoch #150/500, train loss=0.30898284912109375, test loss=nan\n",
      "epoch #200/500, train loss=-5.682104110717773, test loss=nan\n",
      "epoch #250/500, train loss=-50.36501693725586, test loss=nan\n",
      "epoch #300/500, train loss=-69.29701232910156, test loss=nan\n",
      "epoch #350/500, train loss=-69.97430419921875, test loss=nan\n",
      "epoch #400/500, train loss=-99.34700775146484, test loss=nan\n",
      "epoch #450/500, train loss=-87.10760498046875, test loss=nan\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VAE_temp_none_textured_N_10_seed_2\\VAE_temp_none_textured_N_10_seed_2\n",
      "current git commit: 829ee8f4351fe9e326c9d86fd71a52ed65999688\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp VAE_temp_full_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.732718825340271\n",
      "epoch #50/500, loss=0.6612141132354736\n",
      "epoch #100/500, loss=0.3623542785644531\n",
      "epoch #150/500, loss=0.06098340079188347\n",
      "epoch #200/500, loss=0.02180153876543045\n",
      "epoch #250/500, loss=0.01788211241364479\n",
      "epoch #300/500, loss=0.012884342111647129\n",
      "epoch #350/500, loss=0.011097465641796589\n",
      "epoch #400/500, loss=0.009344897232949734\n",
      "epoch #450/500, loss=0.007478659972548485\n",
      "done fitting.\n",
      "pre-training ivi parameters to match q(Z|X)\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/50, loss=22.3986759185791\n",
      "epoch #5/50, loss=20.215473175048828\n",
      "epoch #10/50, loss=17.922895431518555\n",
      "epoch #15/50, loss=16.125885009765625\n",
      "epoch #20/50, loss=14.39932632446289\n",
      "epoch #25/50, loss=12.913956642150879\n",
      "epoch #30/50, loss=11.596734046936035\n",
      "epoch #35/50, loss=10.385394096374512\n",
      "epoch #40/50, loss=9.369192123413086\n",
      "epoch #45/50, loss=8.338047981262207\n",
      "done fitting.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=58.64112854003906, test loss=69.01346588134766\n",
      "epoch #50/500, train loss=41.66969299316406, test loss=176.24765014648438\n",
      "epoch #100/500, train loss=29.61545181274414, test loss=193.9930877685547\n",
      "epoch #150/500, train loss=11.358633041381836, test loss=102.65550231933594\n",
      "epoch #200/500, train loss=-1.662602424621582, test loss=122.27496337890625\n",
      "epoch #250/500, train loss=-17.92452049255371, test loss=230.3706512451172\n",
      "epoch #300/500, train loss=-51.22953796386719, test loss=400.8663330078125\n",
      "epoch #350/500, train loss=-56.47858810424805, test loss=434.2019958496094\n",
      "epoch #400/500, train loss=-78.45645141601562, test loss=483.31585693359375\n",
      "epoch #450/500, train loss=-96.86935424804688, test loss=575.575927734375\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VAE_temp_full_textured_N_10_seed_2\\VAE_temp_full_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp VAE_temp_use_q_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.732718825340271\n",
      "epoch #50/500, loss=0.6612141132354736\n",
      "epoch #100/500, loss=0.3623542785644531\n",
      "epoch #150/500, loss=0.06098340079188347\n",
      "epoch #200/500, loss=0.02180153876543045\n",
      "epoch #250/500, loss=0.01788211241364479\n",
      "epoch #300/500, loss=0.012884342111647129\n",
      "epoch #350/500, loss=0.011097465641796589\n",
      "epoch #400/500, loss=0.009344897232949734\n",
      "epoch #450/500, loss=0.007478659972548485\n",
      "done fitting.\n",
      "- not optimizing ivi parameters because they match q(Z|X) by design.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=34.43788528442383, test loss=34.26179885864258\n",
      "epoch #50/500, train loss=25.128665924072266, test loss=24.981637954711914\n",
      "epoch #100/500, train loss=11.792166709899902, test loss=11.544923782348633\n",
      "epoch #150/500, train loss=1.2113304138183594, test loss=1.6687805652618408\n",
      "epoch #200/500, train loss=-4.822872161865234, test loss=-0.46409910917282104\n",
      "epoch #250/500, train loss=-35.006126403808594, test loss=-20.811784744262695\n",
      "epoch #300/500, train loss=-40.39813995361328, test loss=-41.78355026245117\n",
      "epoch #350/500, train loss=-66.96095275878906, test loss=-56.67451095581055\n",
      "epoch #400/500, train loss=-69.22991180419922, test loss=-70.0596694946289\n",
      "epoch #450/500, train loss=-90.2470932006836, test loss=-86.64115905761719\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VAE_temp_use_q_textured_N_10_seed_2\\VAE_temp_use_q_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp VI_temp_none_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.7381349802017212\n",
      "epoch #50/500, loss=0.7246753573417664\n",
      "epoch #100/500, loss=0.7219492793083191\n",
      "epoch #150/500, loss=0.7116430997848511\n",
      "epoch #200/500, loss=0.7095445990562439\n",
      "epoch #250/500, loss=0.7051737904548645\n",
      "epoch #300/500, loss=0.6916928291320801\n",
      "epoch #350/500, loss=0.6834064722061157\n",
      "epoch #400/500, loss=0.6829222440719604\n",
      "epoch #450/500, loss=0.6696667075157166\n",
      "done fitting.\n",
      "choosing initial values for ivi parameters to match q\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=37.04698944091797, test loss=nan\n",
      "epoch #50/500, train loss=21.62282943725586, test loss=nan\n",
      "epoch #100/500, train loss=13.639232635498047, test loss=nan\n",
      "epoch #150/500, train loss=11.131841659545898, test loss=nan\n",
      "epoch #200/500, train loss=9.830347061157227, test loss=nan\n",
      "epoch #250/500, train loss=8.272404670715332, test loss=nan\n",
      "epoch #300/500, train loss=6.474155426025391, test loss=nan\n",
      "epoch #350/500, train loss=5.052610397338867, test loss=nan\n",
      "epoch #400/500, train loss=0.3415670394897461, test loss=nan\n",
      "epoch #450/500, train loss=-3.891784191131592, test loss=nan\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VI_temp_none_textured_N_10_seed_2\\VI_temp_none_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp VI_temp_full_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.7381349802017212\n",
      "epoch #50/500, loss=0.7246753573417664\n",
      "epoch #100/500, loss=0.7219492793083191\n",
      "epoch #150/500, loss=0.7116430997848511\n",
      "epoch #200/500, loss=0.7095445990562439\n",
      "epoch #250/500, loss=0.7051737904548645\n",
      "epoch #300/500, loss=0.6916928291320801\n",
      "epoch #350/500, loss=0.6834064722061157\n",
      "epoch #400/500, loss=0.6829222440719604\n",
      "epoch #450/500, loss=0.6696667075157166\n",
      "done fitting.\n",
      "pre-training ivi parameters to match q(Z|X)\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/50, loss=0.20448333024978638\n",
      "epoch #5/50, loss=0.18738512694835663\n",
      "epoch #10/50, loss=0.172027587890625\n",
      "epoch #15/50, loss=0.15902315080165863\n",
      "epoch #20/50, loss=0.14693911373615265\n",
      "epoch #25/50, loss=0.135187029838562\n",
      "epoch #30/50, loss=0.1241820827126503\n",
      "epoch #35/50, loss=0.11339956521987915\n",
      "epoch #40/50, loss=0.10246334969997406\n",
      "epoch #45/50, loss=0.09178827702999115\n",
      "done fitting.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=60.85337829589844, test loss=nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #50/500, train loss=29.576946258544922, test loss=nan\n",
      "epoch #100/500, train loss=16.07175064086914, test loss=nan\n",
      "epoch #150/500, train loss=12.268681526184082, test loss=nan\n",
      "epoch #200/500, train loss=11.161633491516113, test loss=nan\n",
      "epoch #250/500, train loss=9.792490005493164, test loss=nan\n",
      "epoch #300/500, train loss=8.814284324645996, test loss=nan\n",
      "epoch #350/500, train loss=6.97736930847168, test loss=nan\n",
      "epoch #400/500, train loss=4.820406913757324, test loss=nan\n",
      "epoch #450/500, train loss=2.8756580352783203, test loss=nan\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VI_temp_full_textured_N_10_seed_2\\VI_temp_full_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp VI_temp_use_q_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "pre-training q(Z|X) to match \\sum_j etaj(xj) + (1-J)eta0\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, loss=0.7381349802017212\n",
      "epoch #50/500, loss=0.7246753573417664\n",
      "epoch #100/500, loss=0.7219492793083191\n",
      "epoch #150/500, loss=0.7116430997848511\n",
      "epoch #200/500, loss=0.7095445990562439\n",
      "epoch #250/500, loss=0.7051737904548645\n",
      "epoch #300/500, loss=0.6916928291320801\n",
      "epoch #350/500, loss=0.6834064722061157\n",
      "epoch #400/500, loss=0.6829222440719604\n",
      "epoch #450/500, loss=0.6696667075157166\n",
      "done fitting.\n",
      "- not optimizing ivi parameters because they match q(Z|X) by design.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=31.523937225341797, test loss=nan\n",
      "epoch #50/500, train loss=20.082473754882812, test loss=nan\n",
      "epoch #100/500, train loss=13.620985984802246, test loss=nan\n",
      "epoch #150/500, train loss=11.218497276306152, test loss=nan\n",
      "epoch #200/500, train loss=9.742254257202148, test loss=nan\n",
      "epoch #250/500, train loss=8.429930686950684, test loss=nan\n",
      "epoch #300/500, train loss=6.808206081390381, test loss=nan\n",
      "epoch #350/500, train loss=4.0384297370910645, test loss=nan\n",
      "epoch #400/500, train loss=1.4910006523132324, test loss=nan\n",
      "epoch #450/500, train loss=-1.9118280410766602, test loss=nan\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\VI_temp_use_q_textured_N_10_seed_2\\VI_temp_use_q_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp amortized_temp_none_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "- not optimizing initial q since q is analytic given RPM!\n",
      "choosing initial values for ivi parameters to match q\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=34.80897903442383, test loss=nan\n",
      "epoch #50/500, train loss=26.442867279052734, test loss=nan\n",
      "epoch #100/500, train loss=6.648927688598633, test loss=nan\n",
      "epoch #150/500, train loss=-26.827625274658203, test loss=nan\n",
      "epoch #200/500, train loss=-41.280517578125, test loss=nan\n",
      "epoch #250/500, train loss=-68.49614715576172, test loss=nan\n",
      "epoch #300/500, train loss=-73.1294937133789, test loss=nan\n",
      "epoch #350/500, train loss=-82.58455657958984, test loss=nan\n",
      "epoch #400/500, train loss=-76.99801635742188, test loss=nan\n",
      "epoch #450/500, train loss=-85.50715637207031, test loss=nan\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\amortized_temp_none_textured_N_10_seed_2\\amortized_temp_none_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp amortized_temp_full_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "- not optimizing initial q since q is analytic given RPM!\n",
      "pre-training ivi parameters to match q(Z|X)\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/50, loss=0.20056208968162537\n",
      "epoch #5/50, loss=0.18053314089775085\n",
      "epoch #10/50, loss=0.16324178874492645\n",
      "epoch #15/50, loss=0.14785845577716827\n",
      "epoch #20/50, loss=0.13317474722862244\n",
      "epoch #25/50, loss=0.11974212527275085\n",
      "epoch #30/50, loss=0.10677259415388107\n",
      "epoch #35/50, loss=0.09431405365467072\n",
      "epoch #40/50, loss=0.08284357935190201\n",
      "epoch #45/50, loss=0.07147683948278427\n",
      "done fitting.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=35.65028762817383, test loss=261.3824157714844\n",
      "epoch #50/500, train loss=26.627727508544922, test loss=248.40176391601562\n",
      "epoch #100/500, train loss=8.604164123535156, test loss=231.0601348876953\n",
      "epoch #150/500, train loss=-20.617294311523438, test loss=354.5164794921875\n",
      "epoch #200/500, train loss=-43.795982360839844, test loss=564.0797119140625\n",
      "epoch #250/500, train loss=-53.886329650878906, test loss=649.2715454101562\n",
      "epoch #300/500, train loss=-60.69140625, test loss=664.1303100585938\n",
      "epoch #350/500, train loss=-82.3658447265625, test loss=708.989990234375\n",
      "epoch #400/500, train loss=-89.2158203125, test loss=735.196044921875\n",
      "epoch #450/500, train loss=-96.1958236694336, test loss=783.9107666015625\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\amortized_temp_full_textured_N_10_seed_2\\amortized_temp_full_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n",
      "\n",
      "\n",
      "running exp amortized_temp_use_q_textured_N_10_seed_2 from directory .\n",
      "\n",
      "\n",
      "(N,T,K,J,batch_size) = (10, 50, 1, 10, 8)\n",
      "initial prior RBF kernel bandwidth:1000.0\n",
      "-  initializing model with *non*-normalized factors f(Z|xj) !\n",
      "- not optimizing initial q since q is analytic given RPM!\n",
      "- not optimizing ivi parameters because they match q(Z|X) by design.\n",
      "\n",
      "\n",
      "fitting model.\n",
      "epoch #0/500, train loss=34.584529876708984, test loss=34.43675994873047\n",
      "epoch #50/500, train loss=26.330387115478516, test loss=26.112247467041016\n",
      "epoch #100/500, train loss=7.171564102172852, test loss=9.308317184448242\n",
      "epoch #150/500, train loss=-27.394075393676758, test loss=-29.730091094970703\n",
      "epoch #200/500, train loss=-50.94474792480469, test loss=-56.879066467285156\n",
      "epoch #250/500, train loss=-63.97141647338867, test loss=-71.28075408935547\n",
      "epoch #300/500, train loss=-72.77169036865234, test loss=-80.1457290649414\n",
      "epoch #350/500, train loss=-59.314781188964844, test loss=-86.48313903808594\n",
      "epoch #400/500, train loss=-74.5375747680664, test loss=-91.04670715332031\n",
      "epoch #450/500, train loss=-86.05453491210938, test loss=-93.84916687011719\n",
      "done fitting.\n",
      "\n",
      "\n",
      "saving results in directory .\\fits\\amortized_temp_use_q_textured_N_10_seed_2\\amortized_temp_use_q_textured_N_10_seed_2\n",
      "current git commit: 36f398795fc979ba340ba576fe14e7d6c176a393\n",
      "done saving results.\n"
     ]
    }
   ],
   "source": [
    "#from exps_poisson_ball import setup_poisson_balls, run_exp_poisson_balls\n",
    "from exps_textured_ball import setup_textured_balls, run_exp_textured_balls\n",
    "\n",
    "rpm_variants = ['VAE', 'VI', 'amortized']  #, 'termporal']\n",
    "amortize_ivis = ['none', 'full', 'use_q']\n",
    "\n",
    "seeds = [2]\n",
    "\n",
    "Ns = [10]\n",
    "batch_sizes = [8]\n",
    "epochses = [500]\n",
    "\n",
    "conf_exp = {\n",
    "    'T' : 50,\n",
    "    'K' : 1,\n",
    "    'J' : 10,\n",
    "    'init_rb_bandwidth' : 1000.0,\n",
    "    #'init_diag_val' : 0.4,\n",
    "    #'init_off_val' : -1.0,\n",
    "    'lr' : 1e-3,\n",
    "    'store_results' : 1,\n",
    "    'scale_th' : 0.15,\n",
    "    'shape_max_0' : 1000,\n",
    "    'ball_sigma2' : 0.01\n",
    "}\n",
    "\n",
    "temporal = 1.\n",
    "iviNatParametrization = 'delta'\n",
    "normalized_factors = False\n",
    "\n",
    "for i in range(len(Ns)):\n",
    "    N, batch_size, epochs = Ns[i], batch_sizes[i], epochses[i]\n",
    "    for seed in seeds:\n",
    "        for rpm_variant in rpm_variants:\n",
    "            for amortize_ivi in amortize_ivis:\n",
    "                run_exp_textured_balls(N=N, model_seed=seed, data_seed=seed,\n",
    "                                       rpm_variant=rpm_variant,temporal=temporal,amortize_ivi=amortize_ivi,\n",
    "                                       epochs=epochs,batch_size=batch_size, \n",
    "                                       optim_init_q=True, optim_init_ivi=True, optim_vae_params=False,\n",
    "                                       iviNatParametrization=iviNatParametrization,\n",
    "                                       normalized_factors=normalized_factors,\n",
    "                                       **conf_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fd00d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tmp = np.ones((8, 50, 2))\n",
    "tmp[:,:,0] = 0.\n",
    "np.max(np.abs(tmp - (tmp.reshape(-1,2)).reshape(8,50,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41331a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_data_external import linear_regression_1D_latent as regLatent\n",
    "from utils_data_external import plot_poisson_balls\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from exps_poisson_ball import init_gaussian_rpm\n",
    "from exps_textured_ball import init_gaussian_rpm\n",
    "\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "\n",
    "N = 100\n",
    "rpm_variant = 'amortized'\n",
    "amortize_ivi = 'use_q'\n",
    "model_seed = 0\n",
    "temporal = True\n",
    "\n",
    "if temporal:\n",
    "    identifier = rpm_variant + '_temp_' + amortize_ivi\n",
    "else:\n",
    "    identifier = rpm_variant + '_' + amortize_ivi        \n",
    "identifier = identifier + '_textured_N_' + str(N) + '_seed_' + str(model_seed)\n",
    "\n",
    "root = os.curdir\n",
    "res_dir = 'fits'\n",
    "fn_base = os.path.join(res_dir, identifier, identifier)\n",
    "\n",
    "data = torch.tensor(np.load(fn_base + '_train_data.npy'))\n",
    "true_latent_ext = torch.tensor(np.load(fn_base + '_train_latents.npy'))\n",
    "\n",
    "#data = torch.tensor(np.load(fn_base + '_test_data.npy'))\n",
    "#true_latent_ext = torch.tensor(np.load(fn_base + '_test_latents.npy'))\n",
    "\n",
    "\n",
    "exp_dict = np.load(fn_base + '_exp_dict.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "N,J,K,T = exp_dict['N'],exp_dict['J'],exp_dict['K'],exp_dict['T']\n",
    "#init_diag_val = exp_dict['init_diag_val']\n",
    "#init_off_val = exp_dict['init_off_val']\n",
    "init_rb_bandwidth = exp_dict['init_rb_bandwidth']\n",
    "\n",
    "ls_train = np.load(fn_base + '_loss_train.npy')\n",
    "ls_test = np.load(fn_base + '_loss_test.npy')\n",
    "\n",
    "xjs = [data[:,j] for j in range(J)]\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n",
    "observations = (torch.stack(xjs, dim=-1),)\n",
    "\n",
    "\n",
    "obs_locs = torch.linspace(0,1,T).reshape(-1,1)\n",
    "model = init_gaussian_rpm(N, J, K, T, pxjs,\n",
    "                        init_rb_bandwidth, obs_locs,\n",
    "                        rpm_variant, temporal, amortize_ivi,\n",
    "                        epochs=0, batch_size=N\n",
    "                       )[0]\n",
    "model.load_state_dict(torch.load(fn_base + '_rpm_state_dict'))\n",
    "\n",
    "\n",
    "prior = model.joint_model[1]\n",
    "eta_0 = prior.nat_param\n",
    "if rpm_variant == 'amortized':    \n",
    "    eta_q, _ = model.comp_eta_q(xjs, eta_0)\n",
    "else: \n",
    "    eta_q = model.comp_eta_q(xjs, eta_0=eta_0, idx_data=np.arange(N))\n",
    "EqtZ = prior.log_partition.nat2meanparam(eta_q)\n",
    "\n",
    "mu = EqtZ[:,:T]\n",
    "sig2 = torch.diagonal(EqtZ[:,T:].reshape(-1,T,T),dim1=-2,dim2=-1) - mu**2\n",
    "\n",
    "latent_true, latent_mean_fit, latent_variance_fit, R2 = regLatent(\n",
    "    latent_true = true_latent_ext,\n",
    "    latent_mean_fit = mu.unsqueeze(-1), \n",
    "    latent_variance_fit = sig2)\n",
    "\n",
    "epochs = exp_dict['epochs']\n",
    "plt.plot(np.linspace(1, epochs, len(ls_train)), ls_train,label='train')\n",
    "plt.plot(np.arange(epochs)+1, ls_test,label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_poisson_balls(observations, \n",
    "                   obs_locs=obs_locs.squeeze(-1), \n",
    "                   latent_mean_fit=latent_mean_fit.squeeze(-1), \n",
    "                   latent_variance_fit=latent_variance_fit)\n",
    "model.joint_model[1].param_.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data_external import sample_textured_balls, plot_poisson_balls\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data_seed = 0\n",
    "N = 10\n",
    "J = 10\n",
    "T = 50\n",
    "\n",
    "np.random.seed(data_seed)\n",
    "torch.manual_seed(data_seed)\n",
    "\n",
    "scale_th = 0.15\n",
    "shape_max_0 = 1000\n",
    "sigma2 = 0.01\n",
    "\n",
    "observations_all, true_latent_all, obs_locs = sample_textured_balls(\n",
    "    num_observation=2*N, dim_observation=J, len_observation=T, \n",
    "    scale_th=scale_th, sigma2=sigma2, shape_max_0=shape_max_0, F=10, omega=0.5)\n",
    "\n",
    "xjs = [observations_all[0][:N,...,j] for j in range(J)]\n",
    "xjs_test = [observations_all[0][N:,...,j] for j in range(J)] \n",
    "true_latent_ext = true_latent_all[:N,:]\n",
    "true_latent_ext_test = true_latent_all[N:,:]\n",
    "\n",
    "from rpm import RPMEmpiricalMarginals\n",
    "pxjs = RPMEmpiricalMarginals(xjs)\n",
    "\n",
    "plot_poisson_balls(observations_all, \n",
    "                   obs_locs=obs_locs.squeeze(-1), \n",
    "                   latent_mean_fit=true_latent_all.squeeze(-1), \n",
    "                   latent_variance_fit=torch.ones_like(true_latent_all))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff74e3e",
   "metadata": {},
   "source": [
    "# construction site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d1f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
